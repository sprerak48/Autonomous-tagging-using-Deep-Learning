{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprerak48/Autonomous-tagging-using-Deep-Learning/blob/Prerak/AT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZAfcIE8zZ5p_",
        "colab_type": "code",
        "outputId": "d98d654e-d6fc-4ed3-9259-29838f2db60d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8e7f3878-7766-41ef-988c-9c786d4df530\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8e7f3878-7766-41ef-988c-9c786d4df530\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iI-B7nIMciHk",
        "colab_type": "code",
        "outputId": "74f65133-b2ca-423a-d960-d7872396657b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h_ormtS5eE2B",
        "colab_type": "code",
        "outputId": "618b842c-25e0-4c58-8a74-0b05687d4b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d stackoverflow/stacksample"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading stacksample.zip to /content\n",
            "100% 1.14G/1.15G [00:12<00:00, 90.0MB/s]\n",
            "100% 1.15G/1.15G [00:12<00:00, 98.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qRIx_AYeVaR",
        "colab_type": "code",
        "outputId": "a0b2d7b1-5076-464e-f77c-dbb19bab26e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip stacksample\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stacksample.zip\n",
            "  inflating: Answers.csv             \n",
            "  inflating: Questions.csv           \n",
            "  inflating: Tags.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D9JatTxP4oXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "import math\n",
        "\n",
        "questions = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "answers = pd.read_csv(\"Answers.csv\", encoding='latin1')\n",
        "tags = pd.read_csv(\"Tags.csv\", encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKueCeSweaK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions = questions.drop([\"OwnerUserId\",\"CreationDate\",\"ClosedDate\"],axis=1)\n",
        "answers = answers.drop([\"Id\",\"OwnerUserId\",\"CreationDate\",\"Score\"],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRIUkFQNfEhv",
        "colab_type": "code",
        "outputId": "d80b3b11-1d34-4003-c43d-d051cdda006d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "print(questions.head())\n",
        "print(answers.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Id  Score                                              Title  \\\n",
            "0   80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "1   90    144  Good branching and merging tutorials for Torto...   \n",
            "2  120     21                                  ASP.NET Site Maps   \n",
            "3  180     53                 Function for creating color wheels   \n",
            "4  260     49  Adding scripting functionality to .NET applica...   \n",
            "\n",
            "                                                Body  \n",
            "0  <p>I've written a database generation script i...  \n",
            "1  <p>Are there any really good tutorials explain...  \n",
            "2  <p>Has anyone got experience creating <strong>...  \n",
            "3  <p>This is something I've pseudo-solved many t...  \n",
            "4  <p>I have a little game written in C#. It uses...  \n",
            "   ParentId                                               Body\n",
            "0        90  <p><a href=\"http://svnbook.red-bean.com/\">Vers...\n",
            "1        80  <p>I wound up using this. It is a kind of a ha...\n",
            "2       180  <p>I've read somewhere the human eye can't dis...\n",
            "3       260  <p>Yes, I thought about that, but I soon figur...\n",
            "4       260  <p><a href=\"http://www.codeproject.com/Article...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "47ettaILffeW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f1 = pd.merge(questions, tags, on='Id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMW8LIsrfw2X",
        "colab_type": "code",
        "outputId": "eb003b94-22d8-4ee8-83eb-4d937c17a64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "print(f1.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Id  Score                                              Title  \\\n",
            "0  80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "1  80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "2  80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "3  90    144  Good branching and merging tutorials for Torto...   \n",
            "4  90    144  Good branching and merging tutorials for Torto...   \n",
            "\n",
            "                                                Body             Tag  \n",
            "0  <p>I've written a database generation script i...            flex  \n",
            "1  <p>I've written a database generation script i...  actionscript-3  \n",
            "2  <p>I've written a database generation script i...             air  \n",
            "3  <p>Are there any really good tutorials explain...             svn  \n",
            "4  <p>Are there any really good tutorials explain...     tortoisesvn  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HbhybzDOf0Yx",
        "colab_type": "code",
        "outputId": "43893195-7d51-44fc-9568-6441813c6a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(document):\n",
        "    document = document.lower() # Convert to lowercase\n",
        "    words = tokenizer.tokenize(document) # Tokenize\n",
        "    words = [w for w in words if not w in stop_words] # Removing stopwords\n",
        "    # Lemmatizing\n",
        "    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n",
        "        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n",
        "    return \" \".join(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2NbaX6MgcFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new = f1['Title'].apply(preprocess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dXOKr4yqllfP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = new.tolist()\n",
        "y = f1[\"Tag\"].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NoMDVQJeAfe3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(y[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LY3q-Isgu3Ma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8')\n",
        "vectorizer.fit(X)\n",
        "vectorizer.transform(X)\n",
        "#vectorizer.transform(y)\n",
        "tfidf_vector_X = vectorizer.transform(X).toarray() #shape - (3,6)\n",
        "tfidf_vector_y = vectorizer.transform(y).toarray() #shape - (3,6)\n",
        "tfidf_vector_X = tfidf_vector_X[:, :, None] #shape - (3,6,1) since LSTM cells expects ndims = 3\n",
        "tfidf_vector_y = tfidf_vector_y[:, :, None] #shape - (3,6,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "befmkkt2z0sI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from keras import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LSTM\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_vector_X, tfidf_vector_Y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYtQ9ohXqr--",
        "colab_type": "code",
        "outputId": "32601614-6a45-4622-df49-9bf098b218f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=6, input_shape = X_train[:,:,None], return_sequences = True))\n",
        "model.add(LSTM(units=6, return_sequences=True))\n",
        "model.add(LSTM(units=6, return_sequences=True))\n",
        "model.add(LSTM(units=1, return_sequences=True, name='output'))\n",
        "model.compile(loss='cosine_proximity', optimizer='sgd', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(X_train, y_train, epochs=1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1fab90fe4500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "lfmmI-Vk5As2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ans_per_question = collections.Counter(answers['ParentId'])\n",
        "answerid,noAnswers= zip(*ans_per_question.most_common())\n",
        "text = \"Avegrage number of answers per question \",np.mean(noAnswers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iu6rUZAI5QaX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N=20\n",
        "plt.bar(range(N), noAnswers[:N], align='center', alpha=0.5)\n",
        "#plt.xticks(y_pos, objects)\n",
        "\n",
        "plt.ylabel('Number of Answers per Question')\n",
        "plt.xlabel('Question Id')\n",
        "plt.title('Distribution of Answers per question ')\n",
        "plt.text(3,400,\"Average answers per question \"+str(math.ceil((np.mean(noAnswers)))))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "liqu0kcO5b2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Top 2 Questions with maximum number of answers \\n\")\n",
        "qid = answerid[:2] \n",
        "\n",
        "for b,id in zip(questions['Body'],questions['Id']):\n",
        "    if id in qid:\n",
        "        #print(id)\n",
        "        print(b)\n",
        "        print(\"................\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZM4qVp35jsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tags(tagCount):\n",
        "    \n",
        "    x,y = zip(*tagCount)\n",
        "\n",
        "    colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "    colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "    area = [i/4000 for i in list(y)]   # 0 to 15 point radiuses\n",
        "    plt.figure(figsize=(9,8))\n",
        "    plt.ylabel(\"Number of question associations\")\n",
        "    for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='o', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "    plt.legend(numpoints=1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0RrXZE95oyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tagCount =  collections.Counter(list(tags['Tag'])).most_common(10)\n",
        "print(tagCount)\n",
        "plot_tags(tagCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1o_ckWoE6L_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "topQuestions =list(zip(questions[\"Id\"],questions[\"Score\"],questions[\"Title\"]))\n",
        "topQuestions.sort(key=lambda x: x[1],reverse=True)\n",
        "for id,s,t in topQuestions[:20]:\n",
        "    #print(\"Question id:\",id)\n",
        "    print(\"Score :\",s)\n",
        "    print(\"Question Title\\t:\",t,'\\n') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eih67Lm7E4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#IDENTIFYING CLUSTERS OF SIMILAR QUESTIONS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from multiprocessing import Pool\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import re\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import scipy.io as scio\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import scipy.spatial.distance as distance\n",
        "import scipy.cluster.hierarchy as hierarchy\n",
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Ez6RDFB7OI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dat = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "dat['Title'].fillna(\"None\", inplace=True)\n",
        "dat['Score'].fillna(0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sg8FwEA7YXD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "selected_ids = np.random.choice(range(dat.shape[0]), 20000, replace=False)\n",
        "sample = dat.loc[selected_ids, :]\n",
        "sample.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-aNIOrcn7mz0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBdbUDYE7ozk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def purify_string(html):\n",
        "    return re.sub('(\\r\\n)+|\\r+|\\n+', \" \", re.sub('<[^<]+?>', '', html))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLk8J0n07uXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = sample.ix[:, 'Body'].apply(purify_string)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_MHVO_I76qq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def combine_title_body(tnb):\n",
        "    return tnb[0] + \" \" + tnb[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhVA9ZEq7-kP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pool(8)\n",
        "combined_corpus = p.map(combine_title_body, zip(dat['Title'], corpus))\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2cNrlkI8FOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "combined_corpus[:2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1RZPu--Y8SSk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()\n",
        "def cond_tokenize(t):\n",
        "    if t is None:\n",
        "        return []\n",
        "    else:\n",
        "        return [lem.lemmatize(w.lower()) for w in word_tokenize(t)]\n",
        "\n",
        "p = Pool(8)\n",
        "tokens = list(p.imap(cond_tokenize, combined_corpus))\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wkE4KtB9OcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pure_tokens = [\" \".join(sent) for sent in tokens]\n",
        "print(tokens[0]) # this are the single lemmatized and stemmed tokens\n",
        "print(pure_tokens[0]) # these are the tokens combined in original form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43YC6wNF9052",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(min_df=1, max_features=2000, stop_words='english', ngram_range=[1, 1], sublinear_tf=True)\n",
        "tfidf = vectorizer.fit_transform(pure_tokens) # this is the vector matrix of the tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GxBZ2-6i97ak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idfs = pd.DataFrame([[v, k] for k, v in vectorizer.vocabulary_.items()], columns=['id', 'word']).sort_values('id')\n",
        "idfs['idf'] = vectorizer.idf_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWoiLhDp98sx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(idfs.sort_values('idf').head(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbOKcI09C6ri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tsvd = TruncatedSVD(n_components=500) # TODO this n_components=500 is a hyperparameter, look into it\n",
        "transformed = tsvd.fit_transform(tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-I4R91ODBXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.sum(tsvd.explained_variance_ratio_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMWx2h1PDHg-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transformed.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnlD02M-DgVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D = distance.pdist(transformed, 'cosine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WagL1YzhD8SH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "L = hierarchy.linkage(D)\n",
        "np.mean(D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GNfCjLsECpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cls = hierarchy.fcluster(L, 0.71, criterion='inconsistent')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Fme4usWEU8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_cls = pd.DataFrame({'Pos': selected_ids, 'Cluster': cls})\n",
        "cnts = df_cls.groupby('Cluster').size().sort_values(ascending=False)\n",
        "cnts.sort_values(ascending=False).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ImQdm5aYEif0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc = pd.concat([sample, df_cls.set_index('Pos')], axis=1)\n",
        "bc.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-sKUng8E6gG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stats = bc.groupby('Cluster')['Score'].describe().unstack()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpNL9wdaFFHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stats.sort_values(ascending=False).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9erJFmPUFnLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.hlines([0], xmin=0, xmax=np.max(stats['count']) + 5, alpha=0.5)\n",
        "plt.vlines([1], ymin=0, ymax=np.max(stats['mean']) + 50, alpha=0.5)\n",
        "plt.scatter(stats['count'], stats['mean'], alpha=0.3)\n",
        "plt.title(\"cluster mean score vs cluster size\")\n",
        "plt.xlabel(\"cluster size\")\n",
        "plt.ylabel(\"mean score\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRNWyWL1HZ4I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[0]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJsIInooHgAj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[1]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJ7_d9jBHwLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[2]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DmuUVoP2GM5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kG7flNqaGdT7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions_df = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "tags_df = pd.read_csv(\"Tags.csv\", encoding='latin1')\n",
        "questions_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbGGD37kHHFr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob as tb\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "from html.parser import HTMLParser\n",
        "import collections\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "dataset_dir = \"../content/\"\n",
        "dataset_dir_questions = \"Questions.csv\"\n",
        "dataset_dir_answers = \"Answers.csv\"\n",
        "dataset_dir_tags = \"Tags.csv\"\n",
        "# for offline run\n",
        "# dataset_dir = \"/home/su/Downloads/stacksample\"\n",
        "# list the files in the dataset directory\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", dataset_dir]).decode(\"utf8\"))\n",
        "cachedStopWords = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuoDUdCgKtnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs= True\n",
        "        self.fed = []\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "    def get_data(self):\n",
        "        return ''.join(self.fed)\n",
        "\n",
        "def strip_tags(html):\n",
        "    s = MLStripper()\n",
        "    s.feed(html)\n",
        "    return s.get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_TvkjYsK3sl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tf(word, blob):\n",
        "    return blob.words.count(word) / len(blob.words)\n",
        "def n_containing(word, bloblist):\n",
        "    return sum(1 for blob in bloblist if word in blob.words)\n",
        "def idf(word, bloblist):\n",
        "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
        "def tfidf(word, blob, bloblist):\n",
        "    return tf(word, blob) * idf(word, bloblist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aKnGap2MK3xa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(str):\n",
        "    return ' '.join([word for word in re.sub(r'[^\\w]', ' ', strip_tags(str)).lower().split() if word not in cachedStopWords and len(word) > 1 and not word.isdigit()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mv9HTSTLvO2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions_df = pd.read_csv(dataset_dir+dataset_dir_questions, encoding='latin1').iloc[::10000, :]\n",
        "answers_df = pd.read_csv(dataset_dir+dataset_dir_answers, encoding='latin1').iloc[::1000, :]\n",
        "tags_df = pd.read_csv(dataset_dir+dataset_dir_tags, encoding='latin1').iloc[::1000, :]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17BItKRoMfX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "questions_df.shape \n",
        "answers_df.shape \n",
        "tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10)\n",
        "answers_df.head(10)\n",
        "tags_df.head(10) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UORXP9K3NUNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "questions_df.shape \n",
        "answers_df.shape \n",
        "tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10).loc[:, 'Title':'Body']\n",
        "answers_df.head(10).loc[:, 'Body':]\n",
        "tags_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TF7V2nNqN0WV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for index, row in questions_df.iterrows():\n",
        "    questions_df.at[index, 'Body']= normalize(row[6])\n",
        "    questions_df.at[index, 'Title']= normalize(row[5])\n",
        "for index, row in answers_df.iterrows():\n",
        "    answers_df.at[index, 'Body']= normalize(row[5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k57NXs6YN9RY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "# questions_df.shape \n",
        "# answers_df.shape \n",
        "# tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10).loc[:, 'Title':'Body']\n",
        "# answers_df.head(10).loc[:, 'Body':]\n",
        "# tags_df.head(10) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqlmLsfsODac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf_dict={}\n",
        "qID_dict={}\n",
        "bloblist=[]\n",
        "idlist=[]\n",
        "\n",
        "for index, row in questions_df.iterrows():\n",
        "    # also append title to text body\n",
        "    bloblist.append(tb(row[6]+\" \"+row[5]))\n",
        "    idlist.append(row[0])\n",
        "\n",
        "for i, blob in enumerate(bloblist):\n",
        "    if i < 5:\n",
        "        print(\"Top words in question ID {}\".format(idlist[i]))\n",
        "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
        "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, score in sorted_words[:5]:\n",
        "        if i < 5:\n",
        "            print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
        "            \n",
        "        # word dict    \n",
        "        if word in tfidf_dict:\n",
        "            tfidf_dict[word].append([idlist[i],round(score, 5)])\n",
        "        else:\n",
        "            tfidf_dict[word] = [[idlist[i],round(score, 5)]]\n",
        "        \n",
        "        # qID dict\n",
        "        if idlist[i] in qID_dict:\n",
        "            qID_dict[idlist[i]].append(word)\n",
        "        else:\n",
        "            lst=[]\n",
        "            lst.append(word)\n",
        "            qID_dict[idlist[i]]=lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2eMGSEdOesL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for k, v in tfidf_dict.items():\n",
        "    print(k, v)\n",
        "    if i == 10:\n",
        "        break\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0IHwJtrOqke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_duplicate(query):\n",
        "    \n",
        "    def top_words(text):\n",
        "        counts = collections.Counter(text.split())\n",
        "        return [elem for elem, _ in counts.most_common(5)]\n",
        "\n",
        "    termList=top_words(query)\n",
        "    \n",
        "    for k, v in qID_dict.items():\n",
        "        if jaccard_similarity_score(termList, qID_dict[k]) >= 0.75:\n",
        "            print(\"Duplicate Question. Question exists with qID: \" + k)\n",
        "            return\n",
        "    \n",
        "print(\"Not a Duplicate Question.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFM_Hvo6PP1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputQ_title=\"What is the most efficient way to deep clone an object in JavaScript?\"\n",
        "inputQ_body=\"\"\"What is the most efficient way to clone a JavaScript object? \n",
        "I've seen obj = eval(uneval(o)); being used, \n",
        "but that's non-standard and only supported by Firefox.\n",
        "I've done things like obj = JSON.parse(JSON.stringify(o)); but question the efficiency. \n",
        "I've also seen recursive copying functions with various flaws. \n",
        "I'm surprised no canonical solution exists.\"\"\"\n",
        "inputQ_tags=\"javascript, json, object\"\n",
        "\n",
        "# normalize\n",
        "normalized_query=normalize(inputQ_title + \" \" + inputQ_body+ \" \" + inputQ_tags)\n",
        "\n",
        "# predict whether duplicate question\n",
        "predict_duplicate(normalized_query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oND2QVhpPkQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags_tally = collections.Counter(tags_df['Tag'])\n",
        "\n",
        "# x = tag name, y = tag frequency\n",
        "x, y = zip(*tags_tally.most_common(10))\n",
        "\n",
        "colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "area = [i/3 for i in list(y)]   # 0 to 15 point radiuses\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 10 most common tags\")\n",
        "for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='v', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "plt.legend(numpoints=1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgvAex2OQAT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ans_per_question = collections.Counter(answers_df['ParentId'])\n",
        "answerid,noAnswers= zip(*ans_per_question.most_common())\n",
        "\n",
        "N=50\n",
        "plt.bar(range(N), noAnswers[:N], align='center', alpha=0.7)\n",
        "#plt.xticks(y_pos, objects)\n",
        "\n",
        "plt.ylabel('Number of Answers per Question')\n",
        "plt.xlabel('Question Id')\n",
        "plt.title('Distribution of Answers per question ')\n",
        "plt.text(10,1.5,\"Average answers per question: \"+str(math.floor((np.mean(noAnswers)))))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2d9WJ8hjXO_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#word2vec embedding\n",
        "import csv\n",
        "outputDatafile = open('Tags.csv', 'r')\n",
        "outputReader = csv.reader(outputDatafile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "print (\"OutputReader datatype :\",type(outputReader))\n",
        "inputData= []\n",
        "\n",
        "for row in outputReader:\n",
        "    inputData.append(row)\n",
        "    \n",
        "\n",
        "    print(inputData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zkiFVifjqrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!NotebookApp.iopub_data_rate_limit=1000000000000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xi1uZ-Bc_we",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "data,count, dictinory, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
        "\n",
        "window_size = 3\n",
        "vector_dim = 300\n",
        "epochs = 25\n",
        "\n",
        "valid_size = 16\n",
        "valid_window = 100\n",
        "valid_examples = np.random.choice(valid_window,valid_size,replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q2Bq0whHfmbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(data, vocab_size, window_size, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "word_target = np.array(word_target,dtype=\"int32\")\n",
        "word_context = np.array(word_context,dtype=\"int32\")\n",
        "\n",
        "print(couples[:10], labels[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ny-MPhz7Ytck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyAyxnRqYtEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuEt9wkoYyc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"Tags.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcHaNP6iY67m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2YiClONZIkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"tags Characters: \", n_chars)\n",
        "print (\"tags Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OLYIcasZcqG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"tags Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sufP0GSnrIgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "questions = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "answers = pd.read_csv(\"Answers.csv\", encoding='latin1')\n",
        "tags = pd.read_csv(\"Tags.csv\", encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYkuv7MGs_t3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions.shape\n",
        "answers.shape\n",
        "tags.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9JvQhdPwBkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np \n",
        "import string\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1b1uilv-wC9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    global EMPTY\n",
        "    EMPTY = ''\n",
        "    \n",
        "    if not isinstance(text, str): \n",
        "        return text\n",
        "    text = re.sub('<pre><code>.*?</code></pre>', EMPTY, text)\n",
        "\n",
        "    def replace_link(match):\n",
        "        return EMPTY if re.match('[a-z]+://', match.group(1)) else match.group(1)\n",
        "    \n",
        "    text = re.sub('<a[^>]+>(.*)</a>', replace_link, text)\n",
        "    return re.sub('<[^>]+>', EMPTY, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9S44qLU3wGJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions['Text'] = questions['Body'].apply(clean_text).str.lower()\n",
        "questions.Text = questions.Text.apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KEpMofS_wWEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "answers['Text'] = answers['Body'].apply(clean_text).str.lower()\n",
        "answers.Text = answers.Text.apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZzV5TEiwiWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'].nunique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7udxsPPEwuE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tags(tagCount):\n",
        "    \n",
        "    x,y = zip(*tagCount)\n",
        "\n",
        "    colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "    colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "    area = [i/4000 for i in list(y)]   # 0 to 15 point radiuses\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.ylabel(\"Number of question associations\")\n",
        "    for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='o', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "    plt.legend(numpoints=1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kaCNm3Yjwvve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "tagCount =  collections.Counter(list(tags['Tag'])).most_common(10)\n",
        "print(tagCount)\n",
        "plot_tags(tagCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILSj9sw2w5or",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags = tags[(tags.Tag == 'c#') | (tags.Tag == 'java') | (tags.Tag == 'php') | (tags.Tag =='javascript') | (tags.Tag =='jquery') | (tags.Tag == 'android') | (tags.Tag == 'c++') | (tags.Tag == 'iphone') | (tags.Tag == 'python') | (tags.Tag == 'asp.net')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7z3_MmMKxJvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_qo0y6Hx1TD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/stack_over_flow_auto_tagging.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "HcmGBy8Mx3I2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"../content/Questions.csv\", encoding=\"ISO-8859-1\")\n",
        "tags = pd.read_csv(\"../content/Tags.csv\", encoding=\"ISO-8859-1\", dtype={'Tag': str})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmrlsFgIvtQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'] = tags['Tag'].astype(str)\n",
        "grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n",
        "grouped_tags.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "emF6pIquv6Dm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped_tags.reset_index()\n",
        "grouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags.values})\n",
        "grouped_tags_final.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4MW7KCOwCRK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)\n",
        "df = df.merge(grouped_tags_final, on='Id')\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tuK_1yowLIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df = df[df['Score']>3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HTQS04K5wRZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "new_df.isnull().mean(axis=0).plot.barh()\n",
        "plt.title(\"Ratio of missing values per columns\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Db_3E-xfwe76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Dupplicate entries: {}'.format(new_df.duplicated().sum()))\n",
        "new_df.drop_duplicates(inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YKRmzK0wjPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmuNDB6Iw4_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: x.split())\n",
        "all_tags = [item for sublist in new_df['Tags'].values for item in sublist]\n",
        "len(all_tags)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pW0Y_YsaxiVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_set = set(all_tags)\n",
        "unique_tags = list(my_set)\n",
        "len(unique_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUCfgJWOxpAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "flat_list = [item for sublist in new_df['Tags'].values for item in sublist]\n",
        "\n",
        "keywords = nltk.FreqDist(flat_list)\n",
        "\n",
        "keywords = nltk.FreqDist(keywords)\n",
        "\n",
        "frequencies_words = keywords.most_common(100)\n",
        "tags_features = [word[0] for word in frequencies_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lH2YrZfsxyL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CjNCyPYx4ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "keywords.plot(100, cumulative=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ECmU4-hx6w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def most_common(tags):\n",
        "    tags_filtered = []\n",
        "    for i in range(0, len(tags)):\n",
        "        if tags[i] in tags_features:\n",
        "            tags_filtered.append(tags[i])\n",
        "    return tags_filtered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NYI-RMEyDYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: most_common(x))\n",
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: x if len(x)>0 else None)\n",
        "new_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gSZ4inXyJOq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df.dropna(subset=['Tags'], inplace=True)\n",
        "new_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PrRRSkPAyNr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: BeautifulSoup(x).get_text()) \n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip(' ')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJxdNPQ4zDxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: clean_text(x)) \n",
        "token=ToktokTokenizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APUrD61Qzb1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WA1lxllCze2o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def strip_list_noempty(mylist):\n",
        "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
        "    return [item for item in newlist if item != '']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDVr65jOzgCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_punct(text): \n",
        "    words=token.tokenize(text)\n",
        "    punctuation_filtered = []\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
        "    for w in words:\n",
        "        if w in tags_features:\n",
        "            punctuation_filtered.append(w)\n",
        "        else:\n",
        "            punctuation_filtered.append(regex.sub('', w))\n",
        "  \n",
        "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
        "        \n",
        "    return ' '.join(map(str, filtered_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLpyBb_uzk2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: clean_punct(x)) \n",
        "new_df['Body'][2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7maDEj-y0lt3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "lemma=WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yPXVSZl1_jA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemitizeWords(text):\n",
        "    words=token.tokenize(text)\n",
        "    listLemma=[]\n",
        "    for w in words:\n",
        "        x=lemma.lemmatize(w, pos=\"v\")\n",
        "        listLemma.append(x)\n",
        "    return ' '.join(map(str, listLemma))\n",
        "\n",
        "def stopWordsRemove(text):\n",
        "    \n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    words=token.tokenize(text)\n",
        "    \n",
        "    filtered = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    return ' '.join(map(str, filtered))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BP6SdPSb2DaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: lemitizeWords(x)) \n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: stopWordsRemove(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7R3D7gr221O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Title'] = new_df['Title'].apply(lambda x: str(x))\n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: clean_text(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: clean_punct(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: lemitizeWords(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: stopWordsRemove(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMbKb21n3ItH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "no_topics = 40\n",
        "text = new_df['Body']\n",
        "vectorizer_train = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n",
        "                                       max_features=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqrAqSfX3Vfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "TF_IDF_matrix = vectorizer_train.fit_transform(text)\n",
        "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50,random_state=11).fit(TF_IDF_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CeHZmmtO44v6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OjkcgC3249Wm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "no_top_words = 20\n",
        "display_topics(lda, vectorizer_train.get_feature_names(), no_top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mepWdzGl5HW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "X1 = new_df['Body']\n",
        "X2 = new_df['Title']\n",
        "y = new_df['Tags']\n",
        "\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "y_bin = multilabel_binarizer.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "joMDLwNL5PGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
        "                                       max_features=1000)\n",
        "\n",
        "vectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
        "                                       max_features=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjIz9QT1_dUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****Model training and testing****"
      ]
    },
    {
      "metadata": {
        "id": "sIwXBtoq5SyR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X1_tfidf = vectorizer_X1.fit_transform(X1)\n",
        "X2_tfidf = vectorizer_X2.fit_transform(X2)\n",
        "\n",
        "#X_tfidf.fit(np.transpose(np.matrix(X1_tfidf)), np.transpose(np.matrix(X2_tfidf)))\n",
        "X_tfidf = np.stack((np.matrix(X1_tfidf),np.matrix(X2_tfidf)), axis=0)\n",
        "#X_tfidf.shape\n",
        "#y_bin.shape\n",
        "print(X_tfidf.shape,y_bin.shape)\n",
        "print(X1_tfidf.shape,X2_tfidf.shape)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IAGrX3QE7cS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def avg_jacard(y_true,y_pred):\n",
        "    \n",
        "    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n",
        "    \n",
        "    return jacard.mean()*100\n",
        "\n",
        "def print_score(y_pred, clf):\n",
        "    print(\"Clf: \", clf.__class__.__name__)\n",
        "    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n",
        "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n",
        "    print(\"---\")    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_9jaI7y7jE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "\n",
        "dummy = DummyClassifier()\n",
        "sgd = SGDClassifier()\n",
        "lr = LogisticRegression()\n",
        "mn = MultinomialNB()\n",
        "svc = LinearSVC()\n",
        "perceptron = Perceptron()\n",
        "pac = PassiveAggressiveClassifier()\n",
        "\n",
        "for classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n",
        "    clf = OneVsRestClassifier(classifier)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print_score(y_pred, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1V9b3BXV-N_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlpc = MLPClassifier()\n",
        "mlpc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlpc.predict(X_test)\n",
        "\n",
        "print_score(y_pred, mlpc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eGvNd_Wz-TEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlpc = MLPClassifier()\n",
        "mlpc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlpc.predict(X_test)\n",
        "\n",
        "print_score(y_pred, mlpc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VX9OhMdq-aHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "param_grid = {'estimator__C':[1,10,100,1000]\n",
        "              }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14bm064P-mXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PAt5QXp--bkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svc = OneVsRestClassifier(LinearSVC())\n",
        "CV_svc = model_selection.GridSearchCV(estimator=svc, param_grid=param_grid, cv= 5, verbose=10, scoring=make_scorer(avg_jacard,greater_is_better=True))\n",
        "CV_svc.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IMtUYRGx_EDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CV_svc.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jghx-9vU_MoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_model = CV_svc.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print_score(y_pred, best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5UB65zN_lyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****Model end****"
      ]
    },
    {
      "metadata": {
        "id": "Nc6DuROD_pI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(y_train.shape[1]):\n",
        "    print(multilabel_binarizer.classes_[i])\n",
        "    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n",
        "    print(\"\")def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"%s: %s\" % (class_label,\n",
        "              \" \".join(feature_names[j] for j in top10)))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n4vlo5xp_urW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"%s: %s\" % (class_label,\n",
        "              \" \".join(feature_names[j] for j in top10)))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wm8pgZ0m_zTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras, hyperopt, tensorflow\n",
        "\n",
        "\n",
        "feature_names = vectorizer_X1.get_feature_names() + vectorizer_X2.get_feature_names()\n",
        "print_top10(feature_names, best_model, multilabel_binarizer.classes_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F4-8L8DABxQL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!NEW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
      ]
    },
    {
      "metadata": {
        "id": "T3ChjcHKBwWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import os\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uwn_Okn8HZVi",
        "colab_type": "code",
        "outputId": "88797a87-c7a2-4762-f2f2-69175e644ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "cell_type": "code",
      "source": [
        "  tag_dtm = vectorizer.fit_transform(tags['tag'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-59aeb583bad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtag_dtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jd-23CkyI3E6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1b13dd30-6c7b-4b4e-f901-eda17edee404"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(document):\n",
        "    document = document.lower() # Convert to lowercase\n",
        "    words = tokenizer.tokenize(document) # Tokenize\n",
        "    words = [w for w in words if not w in stop_words] # Removing stopwords\n",
        "    # Lemmatizing\n",
        "    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n",
        "        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n",
        "    return \" \".join(words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wLlNHI5N-dcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "outputId": "ab1c6dea-5597-44cb-eb58-bfc1fa5333dd"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "questions['Processed Review'] = questions['Title'].apply(preprocess)\n",
        "\n",
        "questions.head(18)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Score</th>\n",
              "      <th>Title</th>\n",
              "      <th>Body</th>\n",
              "      <th>Processed Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80</td>\n",
              "      <td>26</td>\n",
              "      <td>SQLStatement.execute() - multiple queries in o...</td>\n",
              "      <td>&lt;p&gt;I've written a database generation script i...</td>\n",
              "      <td>sqlstatement execute multiple query one statement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90</td>\n",
              "      <td>144</td>\n",
              "      <td>Good branching and merging tutorials for Torto...</td>\n",
              "      <td>&lt;p&gt;Are there any really good tutorials explain...</td>\n",
              "      <td>good branch merge tutorial tortoisesvn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>120</td>\n",
              "      <td>21</td>\n",
              "      <td>ASP.NET Site Maps</td>\n",
              "      <td>&lt;p&gt;Has anyone got experience creating &lt;strong&gt;...</td>\n",
              "      <td>asp net site map</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>180</td>\n",
              "      <td>53</td>\n",
              "      <td>Function for creating color wheels</td>\n",
              "      <td>&lt;p&gt;This is something I've pseudo-solved many t...</td>\n",
              "      <td>function create color wheel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>260</td>\n",
              "      <td>49</td>\n",
              "      <td>Adding scripting functionality to .NET applica...</td>\n",
              "      <td>&lt;p&gt;I have a little game written in C#. It uses...</td>\n",
              "      <td>add script functionality net application</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>330</td>\n",
              "      <td>29</td>\n",
              "      <td>Should I use nested classes in this case?</td>\n",
              "      <td>&lt;p&gt;I am working on a collection of classes use...</td>\n",
              "      <td>use nest class case</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>470</td>\n",
              "      <td>13</td>\n",
              "      <td>Homegrown consumption of web services</td>\n",
              "      <td>&lt;p&gt;I've been writing a few web services for a ...</td>\n",
              "      <td>homegrown consumption web service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>580</td>\n",
              "      <td>21</td>\n",
              "      <td>Deploying SQL Server Databases from Test to Live</td>\n",
              "      <td>&lt;p&gt;I wonder how you guys manage deployment of ...</td>\n",
              "      <td>deploy sql server database test live</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>650</td>\n",
              "      <td>79</td>\n",
              "      <td>Automatically update version number</td>\n",
              "      <td>&lt;p&gt;I would like the version property of my app...</td>\n",
              "      <td>automatically update version number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>810</td>\n",
              "      <td>9</td>\n",
              "      <td>Visual Studio Setup Project - Per User Registr...</td>\n",
              "      <td>&lt;p&gt;I'm trying to maintain a Setup Project in &lt;...</td>\n",
              "      <td>visual studio setup project per user registry set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>930</td>\n",
              "      <td>28</td>\n",
              "      <td>How do I connect to a database and loop over a...</td>\n",
              "      <td>&lt;p&gt;What's the simplest way to connect and quer...</td>\n",
              "      <td>connect database loop recordset c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1010</td>\n",
              "      <td>14</td>\n",
              "      <td>How to get the value of built, encoded ViewState?</td>\n",
              "      <td>&lt;p&gt;I need to grab the base64-encoded represent...</td>\n",
              "      <td>get value build encode viewstate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1040</td>\n",
              "      <td>42</td>\n",
              "      <td>How do I delete a file which is locked by anot...</td>\n",
              "      <td>&lt;p&gt;I'm looking for a way to delete a file whic...</td>\n",
              "      <td>delete file lock another process c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1070</td>\n",
              "      <td>17</td>\n",
              "      <td>Process size on UNIX</td>\n",
              "      <td>&lt;p&gt;What is the correct way to get the process ...</td>\n",
              "      <td>process size unix</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1160</td>\n",
              "      <td>36</td>\n",
              "      <td>Use SVN Revision to label build in CCNET</td>\n",
              "      <td>&lt;p&gt;I am using CCNET on a sample project with S...</td>\n",
              "      <td>use svn revision label build ccnet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1180</td>\n",
              "      <td>17</td>\n",
              "      <td>How to make subdomain user accounts in a webapp</td>\n",
              "      <td>&lt;p&gt;I am looking to allow users to control of s...</td>\n",
              "      <td>make subdomain user account webapp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1300</td>\n",
              "      <td>23</td>\n",
              "      <td>Is nAnt still supported and suitable for .net ...</td>\n",
              "      <td>&lt;p&gt;I am using MSBuild to build my stuff. I wan...</td>\n",
              "      <td>nant still support suitable net v</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1390</td>\n",
              "      <td>18</td>\n",
              "      <td>Is Windows Server 2008 \"Server Core\" appropria...</td>\n",
              "      <td>&lt;p&gt;I'm setting up a dedicated SQL Server 2005 ...</td>\n",
              "      <td>window server server core appropriate sql serv...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id  Score                                              Title  \\\n",
              "0     80     26  SQLStatement.execute() - multiple queries in o...   \n",
              "1     90    144  Good branching and merging tutorials for Torto...   \n",
              "2    120     21                                  ASP.NET Site Maps   \n",
              "3    180     53                 Function for creating color wheels   \n",
              "4    260     49  Adding scripting functionality to .NET applica...   \n",
              "5    330     29          Should I use nested classes in this case?   \n",
              "6    470     13              Homegrown consumption of web services   \n",
              "7    580     21   Deploying SQL Server Databases from Test to Live   \n",
              "8    650     79                Automatically update version number   \n",
              "9    810      9  Visual Studio Setup Project - Per User Registr...   \n",
              "10   930     28  How do I connect to a database and loop over a...   \n",
              "11  1010     14  How to get the value of built, encoded ViewState?   \n",
              "12  1040     42  How do I delete a file which is locked by anot...   \n",
              "13  1070     17                               Process size on UNIX   \n",
              "14  1160     36           Use SVN Revision to label build in CCNET   \n",
              "15  1180     17    How to make subdomain user accounts in a webapp   \n",
              "16  1300     23  Is nAnt still supported and suitable for .net ...   \n",
              "17  1390     18  Is Windows Server 2008 \"Server Core\" appropria...   \n",
              "\n",
              "                                                 Body  \\\n",
              "0   <p>I've written a database generation script i...   \n",
              "1   <p>Are there any really good tutorials explain...   \n",
              "2   <p>Has anyone got experience creating <strong>...   \n",
              "3   <p>This is something I've pseudo-solved many t...   \n",
              "4   <p>I have a little game written in C#. It uses...   \n",
              "5   <p>I am working on a collection of classes use...   \n",
              "6   <p>I've been writing a few web services for a ...   \n",
              "7   <p>I wonder how you guys manage deployment of ...   \n",
              "8   <p>I would like the version property of my app...   \n",
              "9   <p>I'm trying to maintain a Setup Project in <...   \n",
              "10  <p>What's the simplest way to connect and quer...   \n",
              "11  <p>I need to grab the base64-encoded represent...   \n",
              "12  <p>I'm looking for a way to delete a file whic...   \n",
              "13  <p>What is the correct way to get the process ...   \n",
              "14  <p>I am using CCNET on a sample project with S...   \n",
              "15  <p>I am looking to allow users to control of s...   \n",
              "16  <p>I am using MSBuild to build my stuff. I wan...   \n",
              "17  <p>I'm setting up a dedicated SQL Server 2005 ...   \n",
              "\n",
              "                                     Processed Review  \n",
              "0   sqlstatement execute multiple query one statement  \n",
              "1              good branch merge tutorial tortoisesvn  \n",
              "2                                    asp net site map  \n",
              "3                         function create color wheel  \n",
              "4            add script functionality net application  \n",
              "5                                 use nest class case  \n",
              "6                   homegrown consumption web service  \n",
              "7                deploy sql server database test live  \n",
              "8                 automatically update version number  \n",
              "9   visual studio setup project per user registry set  \n",
              "10                  connect database loop recordset c  \n",
              "11                   get value build encode viewstate  \n",
              "12                 delete file lock another process c  \n",
              "13                                  process size unix  \n",
              "14                 use svn revision label build ccnet  \n",
              "15                 make subdomain user account webapp  \n",
              "16                  nant still support suitable net v  \n",
              "17  window server server core appropriate sql serv...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "LKgw97ef_Qws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2839
        },
        "outputId": "9162957b-d150-497e-b72c-79b750d5e072"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(questions['Processed Review'])\n",
        "idf = vectorizer.idf_\n",
        "dic = dict(zip(vectorizer.get_feature_names(), idf))\n",
        "print(dic)\n",
        "Y = vectorizer.fit_transform(questions['Body'])\n",
        "Z = vectorizer.fit_transform(questions['Processed Review'],questions['Title'])\n",
        "A = np.asmatrix(Z)\n",
        "for i in range(18):\n",
        "  print(Z[i])\n",
        "y = A.view()\n",
        "print(y)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "corpus_index = [n for n in questions]\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(index=feature_names, columns=corpus_index)\n",
        "print(df)\n",
        "#Y = datax['Processed Review'].tolist()\n",
        "#Z = datax['Processed Review'].tolist()\n",
        "#print(Y)\n",
        "X = questions['Processed Review'].tolist()\n",
        "Y = questions['Processed Review'].tolist()\n",
        "from keras import Sequential\n",
        "from keras.layers import LSTM\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(X)\n",
        "vectorizer.transform(X)\n",
        "vectorizer.transform(Y)\n",
        "tfidf_vector_X = vectorizer.transform(X).toarray() #shape - (3,6)\n",
        "tfidf_vector_Y = vectorizer.transform(Y).toarray() #shape - (3,6)\n",
        "tfidf_vector_X = tfidf_vector_X[:, :, None] #shape - (3,6,1) since LSTM cells expects ndims = 3\n",
        "tfidf_vector_Y = tfidf_vector_Y[:, :, None] #shape - (3,6,1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_vector_X, tfidf_vector_Y, test_size = 0.2, random_state = 1)\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=6, input_shape = X_train.shape[1:], return_sequences = True))\n",
        "model.add(LSTM(units=6, return_sequences=True))\n",
        "model.add(LSTM(units=6, return_sequences=True))\n",
        "model.add(LSTM(units=1, return_sequences=True, name='output'))\n",
        "model.compile(loss='cosine_proximity', optimizer='sgd', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  (0, 98234)\t0.7403972854309845\n",
            "  (0, 32759)\t0.34151633342722226\n",
            "  (0, 65190)\t0.2669696774955837\n",
            "  (0, 82313)\t0.27453780782769155\n",
            "  (0, 71344)\t0.27795355541816086\n",
            "  (0, 98993)\t0.33358126165947705\n",
            "  (0, 41816)\t0.3713619462235159\n",
            "  (0, 10943)\t0.42006552259255014\n",
            "  (0, 61672)\t0.3826753093113846\n",
            "  (0, 107401)\t0.46160408510271733\n",
            "  (0, 105873)\t0.5710661414285123\n",
            "  (0, 5771)\t0.47711621463328285\n",
            "  (0, 66996)\t0.43322626287932453\n",
            "  (0, 95406)\t0.5849162891271955\n",
            "  (0, 60043)\t0.492491682771692\n",
            "  (0, 37936)\t0.3431760449618338\n",
            "  (0, 20459)\t0.345133432783551\n",
            "  (0, 16972)\t0.4698193474310694\n",
            "  (0, 115101)\t0.7364664938195353\n",
            "  (0, 66996)\t0.3812614448559561\n",
            "  (0, 1271)\t0.36194949547168404\n",
            "  (0, 90414)\t0.4249315486262914\n",
            "  (0, 37945)\t0.6218546599679806\n",
            "  (0, 4674)\t0.395426648384136\n",
            "  (0, 111191)\t0.3008583733873273\n",
            "  (0, 66966)\t0.5745184813821075\n",
            "  (0, 15368)\t0.4391228889796609\n",
            "  (0, 12891)\t0.6217586686868829\n",
            "  (0, 44834)\t0.7404453170587686\n",
            "  (0, 18885)\t0.5293957697907202\n",
            "  (0, 114307)\t0.28777602614044945\n",
            "  (0, 92157)\t0.2977680475676358\n",
            "  (0, 25086)\t0.4808591706041034\n",
            "  (0, 97947)\t0.3339319679641498\n",
            "  (0, 92061)\t0.3325789356657749\n",
            "  (0, 22627)\t0.35165193019541136\n",
            "  (0, 103538)\t0.3828959746555687\n",
            "  (0, 57759)\t0.5257249689251479\n",
            "  (0, 7391)\t0.5819862198638991\n",
            "  (0, 110649)\t0.43568151566671365\n",
            "  (0, 112549)\t0.5244516117499338\n",
            "  (0, 69760)\t0.4431976576289424\n",
            "  (0, 113293)\t0.33195073023488114\n",
            "  (0, 100165)\t0.3224514206884754\n",
            "  (0, 93577)\t0.39856971213526365\n",
            "  (0, 79895)\t0.31065250596267596\n",
            "  (0, 75586)\t0.4052434828637828\n",
            "  (0, 111279)\t0.278095083071693\n",
            "  (0, 85143)\t0.46959688054827486\n",
            "  (0, 92426)\t0.2615159904234235\n",
            "  (0, 22627)\t0.36522909335567477\n",
            "  (0, 18545)\t0.43897121022282326\n",
            "  (0, 58818)\t0.39561997190965686\n",
            "  (0, 84304)\t0.7193030124977261\n",
            "  (0, 39060)\t0.2719431270507198\n",
            "  (0, 111972)\t0.28819363050062524\n",
            "  (0, 11466)\t0.3984055961638797\n",
            "  (0, 30877)\t0.4740630473126491\n",
            "  (0, 113023)\t0.6778853701819312\n",
            "  (0, 24705)\t0.4602892445872718\n",
            "  (0, 34752)\t0.2983924464378353\n",
            "  (0, 58369)\t0.5634323965535076\n",
            "  (0, 3990)\t0.40944394771590154\n",
            "  (0, 79499)\t0.4625963115059514\n",
            "  (0, 79499)\t0.5246922594613639\n",
            "  (0, 95484)\t0.5186544581594799\n",
            "  (0, 109894)\t0.6750522838215017\n",
            "  (0, 111191)\t0.16804874296042224\n",
            "  (0, 11466)\t0.299407524425402\n",
            "  (0, 101434)\t0.3922779960717103\n",
            "  (0, 87540)\t0.4707887414267901\n",
            "  (0, 55451)\t0.34704021112002975\n",
            "  (0, 13312)\t0.6214127299278487\n",
            "  (0, 111279)\t0.3179577542784126\n",
            "  (0, 59667)\t0.31657440199205006\n",
            "  (0, 100367)\t0.5363948486727047\n",
            "  (0, 551)\t0.458413919570243\n",
            "  (0, 114331)\t0.5484712929217329\n",
            "  (0, 66996)\t0.27338990017227416\n",
            "  (0, 66397)\t0.5935739700483543\n",
            "  (0, 99328)\t0.41553430706895944\n",
            "  (0, 101238)\t0.36405950060348313\n",
            "  (0, 100988)\t0.5174164901062696\n",
            "  (0, 97947)\t0.23394439978261727\n",
            "  (0, 92061)\t0.6989894974028753\n",
            "  (0, 115532)\t0.2408458030409512\n",
            "  (0, 19843)\t0.3371461868130069\n",
            "  (0, 4925)\t0.43896831045608675\n",
            "  (0, 49493)\t0.3038359271402309\n",
            "[[<1264216x119025 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 6997814 stored elements in Compressed Sparse Row format>]]\n",
            "                        Id Score Title Body Processed Review\n",
            "aa                     NaN   NaN   NaN  NaN              NaN\n",
            "aaa                    NaN   NaN   NaN  NaN              NaN\n",
            "aaaa                   NaN   NaN   NaN  NaN              NaN\n",
            "aaaaaa                 NaN   NaN   NaN  NaN              NaN\n",
            "aaaaadt                NaN   NaN   NaN  NaN              NaN\n",
            "aaaargh                NaN   NaN   NaN  NaN              NaN\n",
            "aaainc                 NaN   NaN   NaN  NaN              NaN\n",
            "aaannnnna              NaN   NaN   NaN  NaN              NaN\n",
            "aab                    NaN   NaN   NaN  NaN              NaN\n",
            "aabb                   NaN   NaN   NaN  NaN              NaN\n",
            "aabbcc                 NaN   NaN   NaN  NaN              NaN\n",
            "aabbxaabb              NaN   NaN   NaN  NaN              NaN\n",
            "aabsssd                NaN   NaN   NaN  NaN              NaN\n",
            "aac                    NaN   NaN   NaN  NaN              NaN\n",
            "aaccess                NaN   NaN   NaN  NaN              NaN\n",
            "aacdecoder             NaN   NaN   NaN  NaN              NaN\n",
            "aacircle               NaN   NaN   NaN  NaN              NaN\n",
            "aacordion              NaN   NaN   NaN  NaN              NaN\n",
            "aacplus                NaN   NaN   NaN  NaN              NaN\n",
            "aacute                 NaN   NaN   NaN  NaN              NaN\n",
            "aad                    NaN   NaN   NaN  NaN              NaN\n",
            "aadata                 NaN   NaN   NaN  NaN              NaN\n",
            "aading                 NaN   NaN   NaN  NaN              NaN\n",
            "aadl                   NaN   NaN   NaN  NaN              NaN\n",
            "aadsts                 NaN   NaN   NaN  NaN              NaN\n",
            "aaf                    NaN   NaN   NaN  NaN              NaN\n",
            "aafigure               NaN   NaN   NaN  NaN              NaN\n",
            "aaml                   NaN   NaN   NaN  NaN              NaN\n",
            "aan                    NaN   NaN   NaN  NaN              NaN\n",
            "aandroi                NaN   NaN   NaN  NaN              NaN\n",
            "...                    ...   ...   ...  ...              ...\n",
            "zwierze                NaN   NaN   NaN  NaN              NaN\n",
            "zwoptex                NaN   NaN   NaN  NaN              NaN\n",
            "zwreadvirtualmemory    NaN   NaN   NaN  NaN              NaN\n",
            "zwsp                   NaN   NaN   NaN  NaN              NaN\n",
            "zwterminateprocess     NaN   NaN   NaN  NaN              NaN\n",
            "zwyieldexecution       NaN   NaN   NaN  NaN              NaN\n",
            "zx                     NaN   NaN   NaN  NaN              NaN\n",
            "zxcvbn                 NaN   NaN   NaN  NaN              NaN\n",
            "zxing                  NaN   NaN   NaN  NaN              NaN\n",
            "zxingobjc              NaN   NaN   NaN  NaN              NaN\n",
            "zxingqr                NaN   NaN   NaN  NaN              NaN\n",
            "zxingwidget            NaN   NaN   NaN  NaN              NaN\n",
            "zxingwidgetcontroller  NaN   NaN   NaN  NaN              NaN\n",
            "zxjdbc                 NaN   NaN   NaN  NaN              NaN\n",
            "zxp                    NaN   NaN   NaN  NaN              NaN\n",
            "zxpsigncmd             NaN   NaN   NaN  NaN              NaN\n",
            "zxvf                   NaN   NaN   NaN  NaN              NaN\n",
            "zygoteinit             NaN   NaN   NaN  NaN              NaN\n",
            "zylin                  NaN   NaN   NaN  NaN              NaN\n",
            "zynga                  NaN   NaN   NaN  NaN              NaN\n",
            "zynq                   NaN   NaN   NaN  NaN              NaN\n",
            "zyx                    NaN   NaN   NaN  NaN              NaN\n",
            "zz                     NaN   NaN   NaN  NaN              NaN\n",
            "zzah                   NaN   NaN   NaN  NaN              NaN\n",
            "zzb                    NaN   NaN   NaN  NaN              NaN\n",
            "zzim                   NaN   NaN   NaN  NaN              NaN\n",
            "zzqd                   NaN   NaN   NaN  NaN              NaN\n",
            "zzz                    NaN   NaN   NaN  NaN              NaN\n",
            "zzzz                   NaN   NaN   NaN  NaN              NaN\n",
            "zzzzzz                 NaN   NaN   NaN  NaN              NaN\n",
            "\n",
            "[119025 rows x 5 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}