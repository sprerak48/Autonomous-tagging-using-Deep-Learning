{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprerak48/Autonomous-tagging-using-Deep-Learning/blob/Prerak/AT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wBcvAi9TZW_t",
        "colab_type": "code",
        "outputId": "8500e29b-05d9-4a00-97e7-0e1eaad0f8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('Our pythn shared notebook we can code here')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our pythn shared notebook we can code here\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZAfcIE8zZ5p_",
        "colab_type": "code",
        "outputId": "3463b7b7-2e4b-4e04-de0f-94c524f0622e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 40
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0cd6e2cc-511e-41b3-8a72-0ee6efb363bb\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0cd6e2cc-511e-41b3-8a72-0ee6efb363bb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "iI-B7nIMciHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h_ormtS5eE2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d stackoverflow/stacksample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5qRIx_AYeVaR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip stacksample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yod6E02cGnb_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csv_file = input('Enter the name of your input file: ')\n",
        "txt_file = input('Enter the name of your output file: ')\n",
        "\n",
        "text_list = []\n",
        "\n",
        "with open(csv_file, \"r\") as my_input_file:\n",
        "    for line in my_input_file:\n",
        "        line = line.split(\",\", 2)\n",
        "        text_list.append(\" \".join(line))\n",
        "\n",
        "with open(txt_file, \"w\") as my_output_file:\n",
        "    my_output_file.write(\"#1\\n\")\n",
        "    my_output_file.write(\"double({},{})\\n\".format(len(text_list), 2))\n",
        "    for line in text_list:\n",
        "        my_output_file.write(\"  \" + line)\n",
        "    print('File Successfully written.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D9JatTxP4oXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "import math\n",
        "\n",
        "questions = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "answers = pd.read_csv(\"Answers.csv\", encoding='latin1')\n",
        "tags = pd.read_csv(\"Tags.csv\", encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lfmmI-Vk5As2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ans_per_question = collections.Counter(answers['ParentId'])\n",
        "answerid,noAnswers= zip(*ans_per_question.most_common())\n",
        "text = \"Avegrage number of answers per question \",np.mean(noAnswers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iu6rUZAI5QaX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N=20\n",
        "plt.bar(range(N), noAnswers[:N], align='center', alpha=0.5)\n",
        "#plt.xticks(y_pos, objects)\n",
        "\n",
        "plt.ylabel('Number of Answers per Question')\n",
        "plt.xlabel('Question Id')\n",
        "plt.title('Distribution of Answers per question ')\n",
        "plt.text(3,400,\"Average answers per question \"+str(math.ceil((np.mean(noAnswers)))))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "liqu0kcO5b2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Top 2 Questions with maximum number of answers \\n\")\n",
        "qid = answerid[:2] \n",
        "\n",
        "for b,id in zip(questions['Body'],questions['Id']):\n",
        "    if id in qid:\n",
        "        #print(id)\n",
        "        print(b)\n",
        "        print(\"................\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZM4qVp35jsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tags(tagCount):\n",
        "    \n",
        "    x,y = zip(*tagCount)\n",
        "\n",
        "    colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "    colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "    area = [i/4000 for i in list(y)]   # 0 to 15 point radiuses\n",
        "    plt.figure(figsize=(9,8))\n",
        "    plt.ylabel(\"Number of question associations\")\n",
        "    for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='o', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "    plt.legend(numpoints=1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0RrXZE95oyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tagCount =  collections.Counter(list(tags['Tag'])).most_common(10)\n",
        "print(tagCount)\n",
        "plot_tags(tagCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1o_ckWoE6L_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "topQuestions =list(zip(questions[\"Id\"],questions[\"Score\"],questions[\"Title\"]))\n",
        "topQuestions.sort(key=lambda x: x[1],reverse=True)\n",
        "for id,s,t in topQuestions[:20]:\n",
        "    #print(\"Question id:\",id)\n",
        "    print(\"Score :\",s)\n",
        "    print(\"Question Title\\t:\",t,'\\n') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eih67Lm7E4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#IDENTIFYING CLUSTERS OF SIMILAR QUESTIONS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from multiprocessing import Pool\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import re\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import scipy.io as scio\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import scipy.spatial.distance as distance\n",
        "import scipy.cluster.hierarchy as hierarchy\n",
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Ez6RDFB7OI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dat = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "dat['Title'].fillna(\"None\", inplace=True)\n",
        "dat['Score'].fillna(0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sg8FwEA7YXD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "selected_ids = np.random.choice(range(dat.shape[0]), 20000, replace=False)\n",
        "sample = dat.loc[selected_ids, :]\n",
        "sample.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-aNIOrcn7mz0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBdbUDYE7ozk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def purify_string(html):\n",
        "    return re.sub('(\\r\\n)+|\\r+|\\n+', \" \", re.sub('<[^<]+?>', '', html))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLk8J0n07uXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = sample.ix[:, 'Body'].apply(purify_string)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_MHVO_I76qq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def combine_title_body(tnb):\n",
        "    return tnb[0] + \" \" + tnb[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhVA9ZEq7-kP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pool(8)\n",
        "combined_corpus = p.map(combine_title_body, zip(dat['Title'], corpus))\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2cNrlkI8FOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "combined_corpus[:2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1RZPu--Y8SSk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()\n",
        "def cond_tokenize(t):\n",
        "    if t is None:\n",
        "        return []\n",
        "    else:\n",
        "        return [lem.lemmatize(w.lower()) for w in word_tokenize(t)]\n",
        "\n",
        "p = Pool(8)\n",
        "tokens = list(p.imap(cond_tokenize, combined_corpus))\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wkE4KtB9OcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pure_tokens = [\" \".join(sent) for sent in tokens]\n",
        "print(tokens[0]) # this are the single lemmatized and stemmed tokens\n",
        "print(pure_tokens[0]) # these are the tokens combined in original form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43YC6wNF9052",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(min_df=1, max_features=2000, stop_words='english', ngram_range=[1, 1], sublinear_tf=True)\n",
        "tfidf = vectorizer.fit_transform(pure_tokens) # this is the vector matrix of the tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GxBZ2-6i97ak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idfs = pd.DataFrame([[v, k] for k, v in vectorizer.vocabulary_.items()], columns=['id', 'word']).sort_values('id')\n",
        "idfs['idf'] = vectorizer.idf_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWoiLhDp98sx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(idfs.sort_values('idf').head(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbOKcI09C6ri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tsvd = TruncatedSVD(n_components=500) # TODO this n_components=500 is a hyperparameter, look into it\n",
        "transformed = tsvd.fit_transform(tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-I4R91ODBXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.sum(tsvd.explained_variance_ratio_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMWx2h1PDHg-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transformed.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnlD02M-DgVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D = distance.pdist(transformed, 'cosine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WagL1YzhD8SH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "L = hierarchy.linkage(D)\n",
        "np.mean(D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GNfCjLsECpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cls = hierarchy.fcluster(L, 0.71, criterion='inconsistent')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Fme4usWEU8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_cls = pd.DataFrame({'Pos': selected_ids, 'Cluster': cls})\n",
        "cnts = df_cls.groupby('Cluster').size().sort_values(ascending=False)\n",
        "cnts.sort_values(ascending=False).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ImQdm5aYEif0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc = pd.concat([sample, df_cls.set_index('Pos')], axis=1)\n",
        "bc.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-sKUng8E6gG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stats = bc.groupby('Cluster')['Score'].describe().unstack()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpNL9wdaFFHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stats.sort_values(ascending=False).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9erJFmPUFnLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.hlines([0], xmin=0, xmax=np.max(stats['count']) + 5, alpha=0.5)\n",
        "plt.vlines([1], ymin=0, ymax=np.max(stats['mean']) + 50, alpha=0.5)\n",
        "plt.scatter(stats['count'], stats['mean'], alpha=0.3)\n",
        "plt.title(\"cluster mean score vs cluster size\")\n",
        "plt.xlabel(\"cluster size\")\n",
        "plt.ylabel(\"mean score\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRNWyWL1HZ4I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[0]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJsIInooHgAj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[1]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJ7_d9jBHwLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[2]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DmuUVoP2GM5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kG7flNqaGdT7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions_df = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "tags_df = pd.read_csv(\"Tags.csv\", encoding='latin1')\n",
        "questions_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbGGD37kHHFr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob as tb\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "from html.parser import HTMLParser\n",
        "import collections\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "dataset_dir = \"../content/\"\n",
        "dataset_dir_questions = \"Questions.csv\"\n",
        "dataset_dir_answers = \"Answers.csv\"\n",
        "dataset_dir_tags = \"Tags.csv\"\n",
        "# for offline run\n",
        "# dataset_dir = \"/home/su/Downloads/stacksample\"\n",
        "# list the files in the dataset directory\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", dataset_dir]).decode(\"utf8\"))\n",
        "cachedStopWords = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuoDUdCgKtnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs= True\n",
        "        self.fed = []\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "    def get_data(self):\n",
        "        return ''.join(self.fed)\n",
        "\n",
        "def strip_tags(html):\n",
        "    s = MLStripper()\n",
        "    s.feed(html)\n",
        "    return s.get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_TvkjYsK3sl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tf(word, blob):\n",
        "    return blob.words.count(word) / len(blob.words)\n",
        "def n_containing(word, bloblist):\n",
        "    return sum(1 for blob in bloblist if word in blob.words)\n",
        "def idf(word, bloblist):\n",
        "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
        "def tfidf(word, blob, bloblist):\n",
        "    return tf(word, blob) * idf(word, bloblist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aKnGap2MK3xa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(str):\n",
        "    return ' '.join([word for word in re.sub(r'[^\\w]', ' ', strip_tags(str)).lower().split() if word not in cachedStopWords and len(word) > 1 and not word.isdigit()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mv9HTSTLvO2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions_df = pd.read_csv(dataset_dir+dataset_dir_questions, encoding='latin1').iloc[::10000, :]\n",
        "answers_df = pd.read_csv(dataset_dir+dataset_dir_answers, encoding='latin1').iloc[::1000, :]\n",
        "tags_df = pd.read_csv(dataset_dir+dataset_dir_tags, encoding='latin1').iloc[::1000, :]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17BItKRoMfX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "questions_df.shape \n",
        "answers_df.shape \n",
        "tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10)\n",
        "answers_df.head(10)\n",
        "tags_df.head(10) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UORXP9K3NUNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "questions_df.shape \n",
        "answers_df.shape \n",
        "tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10).loc[:, 'Title':'Body']\n",
        "answers_df.head(10).loc[:, 'Body':]\n",
        "tags_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TF7V2nNqN0WV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for index, row in questions_df.iterrows():\n",
        "    questions_df.at[index, 'Body']= normalize(row[6])\n",
        "    questions_df.at[index, 'Title']= normalize(row[5])\n",
        "for index, row in answers_df.iterrows():\n",
        "    answers_df.at[index, 'Body']= normalize(row[5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k57NXs6YN9RY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "# questions_df.shape \n",
        "# answers_df.shape \n",
        "# tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10).loc[:, 'Title':'Body']\n",
        "# answers_df.head(10).loc[:, 'Body':]\n",
        "# tags_df.head(10) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqlmLsfsODac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf_dict={}\n",
        "qID_dict={}\n",
        "bloblist=[]\n",
        "idlist=[]\n",
        "\n",
        "for index, row in questions_df.iterrows():\n",
        "    # also append title to text body\n",
        "    bloblist.append(tb(row[6]+\" \"+row[5]))\n",
        "    idlist.append(row[0])\n",
        "\n",
        "for i, blob in enumerate(bloblist):\n",
        "    if i < 5:\n",
        "        print(\"Top words in question ID {}\".format(idlist[i]))\n",
        "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
        "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, score in sorted_words[:5]:\n",
        "        if i < 5:\n",
        "            print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
        "            \n",
        "        # word dict    \n",
        "        if word in tfidf_dict:\n",
        "            tfidf_dict[word].append([idlist[i],round(score, 5)])\n",
        "        else:\n",
        "            tfidf_dict[word] = [[idlist[i],round(score, 5)]]\n",
        "        \n",
        "        # qID dict\n",
        "        if idlist[i] in qID_dict:\n",
        "            qID_dict[idlist[i]].append(word)\n",
        "        else:\n",
        "            lst=[]\n",
        "            lst.append(word)\n",
        "            qID_dict[idlist[i]]=lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2eMGSEdOesL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for k, v in tfidf_dict.items():\n",
        "    print(k, v)\n",
        "    if i == 10:\n",
        "        break\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0IHwJtrOqke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_duplicate(query):\n",
        "    \n",
        "    def top_words(text):\n",
        "        counts = collections.Counter(text.split())\n",
        "        return [elem for elem, _ in counts.most_common(5)]\n",
        "\n",
        "    termList=top_words(query)\n",
        "    \n",
        "    for k, v in qID_dict.items():\n",
        "        if jaccard_similarity_score(termList, qID_dict[k]) >= 0.75:\n",
        "            print(\"Duplicate Question. Question exists with qID: \" + k)\n",
        "            return\n",
        "    \n",
        "print(\"Not a Duplicate Question.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFM_Hvo6PP1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputQ_title=\"What is the most efficient way to deep clone an object in JavaScript?\"\n",
        "inputQ_body=\"\"\"What is the most efficient way to clone a JavaScript object? \n",
        "I've seen obj = eval(uneval(o)); being used, \n",
        "but that's non-standard and only supported by Firefox.\n",
        "I've done things like obj = JSON.parse(JSON.stringify(o)); but question the efficiency. \n",
        "I've also seen recursive copying functions with various flaws. \n",
        "I'm surprised no canonical solution exists.\"\"\"\n",
        "inputQ_tags=\"javascript, json, object\"\n",
        "\n",
        "# normalize\n",
        "normalized_query=normalize(inputQ_title + \" \" + inputQ_body+ \" \" + inputQ_tags)\n",
        "\n",
        "# predict whether duplicate question\n",
        "predict_duplicate(normalized_query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oND2QVhpPkQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags_tally = collections.Counter(tags_df['Tag'])\n",
        "\n",
        "# x = tag name, y = tag frequency\n",
        "x, y = zip(*tags_tally.most_common(10))\n",
        "\n",
        "colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "area = [i/3 for i in list(y)]   # 0 to 15 point radiuses\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 10 most common tags\")\n",
        "for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='v', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "plt.legend(numpoints=1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgvAex2OQAT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ans_per_question = collections.Counter(answers_df['ParentId'])\n",
        "answerid,noAnswers= zip(*ans_per_question.most_common())\n",
        "\n",
        "N=50\n",
        "plt.bar(range(N), noAnswers[:N], align='center', alpha=0.7)\n",
        "#plt.xticks(y_pos, objects)\n",
        "\n",
        "plt.ylabel('Number of Answers per Question')\n",
        "plt.xlabel('Question Id')\n",
        "plt.title('Distribution of Answers per question ')\n",
        "plt.text(10,1.5,\"Average answers per question: \"+str(math.floor((np.mean(noAnswers)))))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2d9WJ8hjXO_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#word2vec embedding\n",
        "import csv\n",
        "outputDatafile = open('Tags.csv', 'r')\n",
        "outputReader = csv.reader(outputDatafile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "print (\"OutputReader datatype :\",type(outputReader))\n",
        "inputData= []\n",
        "\n",
        "for row in outputReader:\n",
        "    inputData.append(row)\n",
        "    \n",
        "\n",
        "    print(inputData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zkiFVifjqrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!NotebookApp.iopub_data_rate_limit=1000000000000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xi1uZ-Bc_we",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "data,count, dictinory, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
        "\n",
        "window_size = 3\n",
        "vector_dim = 300\n",
        "epochs = 25\n",
        "\n",
        "valid_size = 16\n",
        "valid_window = 100\n",
        "valid_examples = np.random.choice(valid_window,valid_size,replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q2Bq0whHfmbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(data, vocab_size, window_size, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "word_target = np.array(word_target,dtype=\"int32\")\n",
        "word_context = np.array(word_context,dtype=\"int32\")\n",
        "\n",
        "print(couples[:10], labels[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ny-MPhz7Ytck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyAyxnRqYtEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuEt9wkoYyc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"Tags.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcHaNP6iY67m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2YiClONZIkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"tags Characters: \", n_chars)\n",
        "print (\"tags Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OLYIcasZcqG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"tags Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sufP0GSnrIgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "questions = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "answers = pd.read_csv(\"Answers.csv\", encoding='latin1')\n",
        "tags = pd.read_csv(\"Tags.csv\", encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYkuv7MGs_t3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions.shape\n",
        "answers.shape\n",
        "tags.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9JvQhdPwBkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np \n",
        "import string\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1b1uilv-wC9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    global EMPTY\n",
        "    EMPTY = ''\n",
        "    \n",
        "    if not isinstance(text, str): \n",
        "        return text\n",
        "    text = re.sub('<pre><code>.*?</code></pre>', EMPTY, text)\n",
        "\n",
        "    def replace_link(match):\n",
        "        return EMPTY if re.match('[a-z]+://', match.group(1)) else match.group(1)\n",
        "    \n",
        "    text = re.sub('<a[^>]+>(.*)</a>', replace_link, text)\n",
        "    return re.sub('<[^>]+>', EMPTY, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9S44qLU3wGJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions['Text'] = questions['Body'].apply(clean_text).str.lower()\n",
        "questions.Text = questions.Text.apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KEpMofS_wWEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "answers['Text'] = answers['Body'].apply(clean_text).str.lower()\n",
        "answers.Text = answers.Text.apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZzV5TEiwiWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'].nunique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7udxsPPEwuE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tags(tagCount):\n",
        "    \n",
        "    x,y = zip(*tagCount)\n",
        "\n",
        "    colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "    colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "    area = [i/4000 for i in list(y)]   # 0 to 15 point radiuses\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.ylabel(\"Number of question associations\")\n",
        "    for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='o', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "    plt.legend(numpoints=1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kaCNm3Yjwvve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "tagCount =  collections.Counter(list(tags['Tag'])).most_common(10)\n",
        "print(tagCount)\n",
        "plot_tags(tagCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILSj9sw2w5or",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags = tags[(tags.Tag == 'c#') | (tags.Tag == 'java') | (tags.Tag == 'php') | (tags.Tag =='javascript') | (tags.Tag =='jquery') | (tags.Tag == 'android') | (tags.Tag == 'c++') | (tags.Tag == 'iphone') | (tags.Tag == 'python') | (tags.Tag == 'asp.net')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7z3_MmMKxJvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_qo0y6Hx1TD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/stack_over_flow_auto_tagging.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "HcmGBy8Mx3I2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"../content/Questions.csv\", encoding=\"ISO-8859-1\")\n",
        "tags = pd.read_csv(\"../content/Tags.csv\", encoding=\"ISO-8859-1\", dtype={'Tag': str})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmrlsFgIvtQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'] = tags['Tag'].astype(str)\n",
        "grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n",
        "grouped_tags.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "emF6pIquv6Dm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped_tags.reset_index()\n",
        "grouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags.values})\n",
        "grouped_tags_final.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4MW7KCOwCRK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)\n",
        "df = df.merge(grouped_tags_final, on='Id')\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tuK_1yowLIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df = df[df['Score']>3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HTQS04K5wRZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "new_df.isnull().mean(axis=0).plot.barh()\n",
        "plt.title(\"Ratio of missing values per columns\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Db_3E-xfwe76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Dupplicate entries: {}'.format(new_df.duplicated().sum()))\n",
        "new_df.drop_duplicates(inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YKRmzK0wjPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmuNDB6Iw4_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: x.split())\n",
        "all_tags = [item for sublist in new_df['Tags'].values for item in sublist]\n",
        "len(all_tags)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pW0Y_YsaxiVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_set = set(all_tags)\n",
        "unique_tags = list(my_set)\n",
        "len(unique_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUCfgJWOxpAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "flat_list = [item for sublist in new_df['Tags'].values for item in sublist]\n",
        "\n",
        "keywords = nltk.FreqDist(flat_list)\n",
        "\n",
        "keywords = nltk.FreqDist(keywords)\n",
        "\n",
        "frequencies_words = keywords.most_common(100)\n",
        "tags_features = [word[0] for word in frequencies_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lH2YrZfsxyL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CjNCyPYx4ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "keywords.plot(100, cumulative=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ECmU4-hx6w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def most_common(tags):\n",
        "    tags_filtered = []\n",
        "    for i in range(0, len(tags)):\n",
        "        if tags[i] in tags_features:\n",
        "            tags_filtered.append(tags[i])\n",
        "    return tags_filtered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NYI-RMEyDYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: most_common(x))\n",
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: x if len(x)>0 else None)\n",
        "new_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gSZ4inXyJOq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df.dropna(subset=['Tags'], inplace=True)\n",
        "new_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PrRRSkPAyNr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: BeautifulSoup(x).get_text()) \n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip(' ')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJxdNPQ4zDxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: clean_text(x)) \n",
        "token=ToktokTokenizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APUrD61Qzb1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WA1lxllCze2o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def strip_list_noempty(mylist):\n",
        "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
        "    return [item for item in newlist if item != '']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDVr65jOzgCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_punct(text): \n",
        "    words=token.tokenize(text)\n",
        "    punctuation_filtered = []\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
        "    for w in words:\n",
        "        if w in tags_features:\n",
        "            punctuation_filtered.append(w)\n",
        "        else:\n",
        "            punctuation_filtered.append(regex.sub('', w))\n",
        "  \n",
        "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
        "        \n",
        "    return ' '.join(map(str, filtered_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLpyBb_uzk2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: clean_punct(x)) \n",
        "new_df['Body'][2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7maDEj-y0lt3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "lemma=WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yPXVSZl1_jA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemitizeWords(text):\n",
        "    words=token.tokenize(text)\n",
        "    listLemma=[]\n",
        "    for w in words:\n",
        "        x=lemma.lemmatize(w, pos=\"v\")\n",
        "        listLemma.append(x)\n",
        "    return ' '.join(map(str, listLemma))\n",
        "\n",
        "def stopWordsRemove(text):\n",
        "    \n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    words=token.tokenize(text)\n",
        "    \n",
        "    filtered = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    return ' '.join(map(str, filtered))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BP6SdPSb2DaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: lemitizeWords(x)) \n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: stopWordsRemove(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7R3D7gr221O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Title'] = new_df['Title'].apply(lambda x: str(x))\n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: clean_text(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: clean_punct(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: lemitizeWords(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: stopWordsRemove(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMbKb21n3ItH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "no_topics = 40\n",
        "text = new_df['Body']\n",
        "vectorizer_train = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n",
        "                                       max_features=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqrAqSfX3Vfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "TF_IDF_matrix = vectorizer_train.fit_transform(text)\n",
        "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50,random_state=11).fit(TF_IDF_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CeHZmmtO44v6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OjkcgC3249Wm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "no_top_words = 20\n",
        "display_topics(lda, vectorizer_train.get_feature_names(), no_top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mepWdzGl5HW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "X1 = new_df['Body']\n",
        "X2 = new_df['Title']\n",
        "y = new_df['Tags']\n",
        "\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "y_bin = multilabel_binarizer.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "joMDLwNL5PGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
        "                                       max_features=1000)\n",
        "\n",
        "vectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
        "                                       max_features=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjIz9QT1_dUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****Model training and testing****"
      ]
    },
    {
      "metadata": {
        "id": "sIwXBtoq5SyR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X1_tfidf = vectorizer_X1.fit_transform(X1)\n",
        "X2_tfidf = vectorizer_X2.fit_transform(X2)\n",
        "\n",
        "#X_tfidf.fit(np.transpose(np.matrix(X1_tfidf)), np.transpose(np.matrix(X2_tfidf)))\n",
        "X_tfidf = np.stack((np.matrix(X1_tfidf),np.matrix(X2_tfidf)), axis=0)\n",
        "#X_tfidf.shape\n",
        "#y_bin.shape\n",
        "print(X_tfidf.shape,y_bin.shape)\n",
        "print(X1_tfidf.shape,X2_tfidf.shape)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IAGrX3QE7cS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def avg_jacard(y_true,y_pred):\n",
        "    \n",
        "    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n",
        "    \n",
        "    return jacard.mean()*100\n",
        "\n",
        "def print_score(y_pred, clf):\n",
        "    print(\"Clf: \", clf.__class__.__name__)\n",
        "    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n",
        "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n",
        "    print(\"---\")    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_9jaI7y7jE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "\n",
        "dummy = DummyClassifier()\n",
        "sgd = SGDClassifier()\n",
        "lr = LogisticRegression()\n",
        "mn = MultinomialNB()\n",
        "svc = LinearSVC()\n",
        "perceptron = Perceptron()\n",
        "pac = PassiveAggressiveClassifier()\n",
        "\n",
        "for classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n",
        "    clf = OneVsRestClassifier(classifier)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print_score(y_pred, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1V9b3BXV-N_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlpc = MLPClassifier()\n",
        "mlpc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlpc.predict(X_test)\n",
        "\n",
        "print_score(y_pred, mlpc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eGvNd_Wz-TEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlpc = MLPClassifier()\n",
        "mlpc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlpc.predict(X_test)\n",
        "\n",
        "print_score(y_pred, mlpc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VX9OhMdq-aHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "param_grid = {'estimator__C':[1,10,100,1000]\n",
        "              }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14bm064P-mXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PAt5QXp--bkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svc = OneVsRestClassifier(LinearSVC())\n",
        "CV_svc = model_selection.GridSearchCV(estimator=svc, param_grid=param_grid, cv= 5, verbose=10, scoring=make_scorer(avg_jacard,greater_is_better=True))\n",
        "CV_svc.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IMtUYRGx_EDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CV_svc.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jghx-9vU_MoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_model = CV_svc.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print_score(y_pred, best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5UB65zN_lyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****Model end****"
      ]
    },
    {
      "metadata": {
        "id": "Nc6DuROD_pI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(y_train.shape[1]):\n",
        "    print(multilabel_binarizer.classes_[i])\n",
        "    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n",
        "    print(\"\")def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"%s: %s\" % (class_label,\n",
        "              \" \".join(feature_names[j] for j in top10)))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n4vlo5xp_urW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"%s: %s\" % (class_label,\n",
        "              \" \".join(feature_names[j] for j in top10)))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wm8pgZ0m_zTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras, hyperopt, tensorflow\n",
        "\n",
        "\n",
        "feature_names = vectorizer_X1.get_feature_names() + vectorizer_X2.get_feature_names()\n",
        "print_top10(feature_names, best_model, multilabel_binarizer.classes_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}