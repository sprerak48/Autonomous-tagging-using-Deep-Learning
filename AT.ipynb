{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sprerak48/Autonomous-tagging-using-Deep-Learning/blob/Prerak/AT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZAfcIE8zZ5p_",
        "colab_type": "code",
        "outputId": "3540626f-d8e6-4b96-8d49-e73a3cf87714",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6de2e4a8-129d-4db1-a40d-f80941ec1a22\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6de2e4a8-129d-4db1-a40d-f80941ec1a22\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iI-B7nIMciHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "76d06af6-0638-4b70-94e5-286b3d08b0f1"
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.3.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (3.0.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: text-unidecode==1.2 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h_ormtS5eE2B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5e68f8cd-ee9a-48b9-a788-9e46b2f8f23c"
      },
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d stackoverflow/stacksample"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading stacksample.zip to /content\n",
            "100% 1.14G/1.15G [00:07<00:00, 192MB/s]\n",
            "100% 1.15G/1.15G [00:07<00:00, 165MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5qRIx_AYeVaR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1d523ff4-5cdc-4bba-9b7a-8b96d9353533"
      },
      "cell_type": "code",
      "source": [
        "!unzip stacksample\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stacksample.zip\n",
            "  inflating: Answers.csv             \n",
            "  inflating: Questions.csv           \n",
            "  inflating: Tags.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D9JatTxP4oXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "import math\n",
        "\n",
        "questions = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "answers = pd.read_csv(\"Answers.csv\", encoding='latin1')\n",
        "tags = pd.read_csv(\"Tags.csv\", encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKueCeSweaK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions = questions.drop([\"OwnerUserId\",\"CreationDate\",\"ClosedDate\"],axis=1)\n",
        "answers = answers.drop([\"Id\",\"OwnerUserId\",\"CreationDate\",\"Score\"],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRIUkFQNfEhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "d80b3b11-1d34-4003-c43d-d051cdda006d"
      },
      "cell_type": "code",
      "source": [
        "print(questions.head())\n",
        "print(answers.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Id  Score                                              Title  \\\n",
            "0   80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "1   90    144  Good branching and merging tutorials for Torto...   \n",
            "2  120     21                                  ASP.NET Site Maps   \n",
            "3  180     53                 Function for creating color wheels   \n",
            "4  260     49  Adding scripting functionality to .NET applica...   \n",
            "\n",
            "                                                Body  \n",
            "0  <p>I've written a database generation script i...  \n",
            "1  <p>Are there any really good tutorials explain...  \n",
            "2  <p>Has anyone got experience creating <strong>...  \n",
            "3  <p>This is something I've pseudo-solved many t...  \n",
            "4  <p>I have a little game written in C#. It uses...  \n",
            "   ParentId                                               Body\n",
            "0        90  <p><a href=\"http://svnbook.red-bean.com/\">Vers...\n",
            "1        80  <p>I wound up using this. It is a kind of a ha...\n",
            "2       180  <p>I've read somewhere the human eye can't dis...\n",
            "3       260  <p>Yes, I thought about that, but I soon figur...\n",
            "4       260  <p><a href=\"http://www.codeproject.com/Article...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "47ettaILffeW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f1 = pd.merge(questions, tags, on='Id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMW8LIsrfw2X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "eb003b94-22d8-4ee8-83eb-4d937c17a64d"
      },
      "cell_type": "code",
      "source": [
        "print(f1.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Id  Score                                              Title  \\\n",
            "0  80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "1  80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "2  80     26  SQLStatement.execute() - multiple queries in o...   \n",
            "3  90    144  Good branching and merging tutorials for Torto...   \n",
            "4  90    144  Good branching and merging tutorials for Torto...   \n",
            "\n",
            "                                                Body             Tag  \n",
            "0  <p>I've written a database generation script i...            flex  \n",
            "1  <p>I've written a database generation script i...  actionscript-3  \n",
            "2  <p>I've written a database generation script i...             air  \n",
            "3  <p>Are there any really good tutorials explain...             svn  \n",
            "4  <p>Are there any really good tutorials explain...     tortoisesvn  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HbhybzDOf0Yx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "43893195-7d51-44fc-9568-6441813c6a32"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(document):\n",
        "    document = document.lower() # Convert to lowercase\n",
        "    words = tokenizer.tokenize(document) # Tokenize\n",
        "    words = [w for w in words if not w in stop_words] # Removing stopwords\n",
        "    # Lemmatizing\n",
        "    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n",
        "        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n",
        "    return \" \".join(words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E2NbaX6MgcFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new = f1['Title'].apply(preprocess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dXOKr4yqllfP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = new.tolist()\n",
        "y = f1[\"Tag\"].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NoMDVQJeAfe3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(y[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LY3q-Isgu3Ma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8')\n",
        "vectorizer.fit(X)\n",
        "vectorizer.transform(X)\n",
        "#vectorizer.transform(y)\n",
        "tfidf_vector_X = vectorizer.transform(X).toarray() #shape - (3,6)\n",
        "tfidf_vector_y = vectorizer.transform(y).toarray() #shape - (3,6)\n",
        "tfidf_vector_X = tfidf_vector_X[:, :, None] #shape - (3,6,1) since LSTM cells expects ndims = 3\n",
        "tfidf_vector_y = tfidf_vector_y[:, :, None] #shape - (3,6,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "befmkkt2z0sI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from keras import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import LSTM\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_vector_X, tfidf_vector_Y, test_size = 0.2, random_state = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYtQ9ohXqr--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "32601614-6a45-4622-df49-9bf098b218f1"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(units=6, input_shape = X_train[:,:,None], return_sequences = True))\n",
        "model.add(LSTM(units=6, return_sequences=True))\n",
        "model.add(LSTM(units=6, return_sequences=True))\n",
        "model.add(LSTM(units=1, return_sequences=True, name='output'))\n",
        "model.compile(loss='cosine_proximity', optimizer='sgd', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(X_train, y_train, epochs=1, verbose=1)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1fab90fe4500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "lfmmI-Vk5As2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ans_per_question = collections.Counter(answers['ParentId'])\n",
        "answerid,noAnswers= zip(*ans_per_question.most_common())\n",
        "text = \"Avegrage number of answers per question \",np.mean(noAnswers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iu6rUZAI5QaX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N=20\n",
        "plt.bar(range(N), noAnswers[:N], align='center', alpha=0.5)\n",
        "#plt.xticks(y_pos, objects)\n",
        "\n",
        "plt.ylabel('Number of Answers per Question')\n",
        "plt.xlabel('Question Id')\n",
        "plt.title('Distribution of Answers per question ')\n",
        "plt.text(3,400,\"Average answers per question \"+str(math.ceil((np.mean(noAnswers)))))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "liqu0kcO5b2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Top 2 Questions with maximum number of answers \\n\")\n",
        "qid = answerid[:2] \n",
        "\n",
        "for b,id in zip(questions['Body'],questions['Id']):\n",
        "    if id in qid:\n",
        "        #print(id)\n",
        "        print(b)\n",
        "        print(\"................\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZM4qVp35jsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tags(tagCount):\n",
        "    \n",
        "    x,y = zip(*tagCount)\n",
        "\n",
        "    colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "    colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "    area = [i/4000 for i in list(y)]   # 0 to 15 point radiuses\n",
        "    plt.figure(figsize=(9,8))\n",
        "    plt.ylabel(\"Number of question associations\")\n",
        "    for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='o', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "    plt.legend(numpoints=1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j0RrXZE95oyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tagCount =  collections.Counter(list(tags['Tag'])).most_common(10)\n",
        "print(tagCount)\n",
        "plot_tags(tagCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1o_ckWoE6L_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "topQuestions =list(zip(questions[\"Id\"],questions[\"Score\"],questions[\"Title\"]))\n",
        "topQuestions.sort(key=lambda x: x[1],reverse=True)\n",
        "for id,s,t in topQuestions[:20]:\n",
        "    #print(\"Question id:\",id)\n",
        "    print(\"Score :\",s)\n",
        "    print(\"Question Title\\t:\",t,'\\n') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eih67Lm7E4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#IDENTIFYING CLUSTERS OF SIMILAR QUESTIONS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from multiprocessing import Pool\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import re\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import scipy.io as scio\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import scipy.spatial.distance as distance\n",
        "import scipy.cluster.hierarchy as hierarchy\n",
        "from scipy.stats import pearsonr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Ez6RDFB7OI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dat = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "dat['Title'].fillna(\"None\", inplace=True)\n",
        "dat['Score'].fillna(0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sg8FwEA7YXD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "selected_ids = np.random.choice(range(dat.shape[0]), 20000, replace=False)\n",
        "sample = dat.loc[selected_ids, :]\n",
        "sample.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-aNIOrcn7mz0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sample.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBdbUDYE7ozk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def purify_string(html):\n",
        "    return re.sub('(\\r\\n)+|\\r+|\\n+', \" \", re.sub('<[^<]+?>', '', html))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLk8J0n07uXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corpus = sample.ix[:, 'Body'].apply(purify_string)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_MHVO_I76qq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def combine_title_body(tnb):\n",
        "    return tnb[0] + \" \" + tnb[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhVA9ZEq7-kP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = Pool(8)\n",
        "combined_corpus = p.map(combine_title_body, zip(dat['Title'], corpus))\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2cNrlkI8FOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "combined_corpus[:2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1RZPu--Y8SSk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()\n",
        "def cond_tokenize(t):\n",
        "    if t is None:\n",
        "        return []\n",
        "    else:\n",
        "        return [lem.lemmatize(w.lower()) for w in word_tokenize(t)]\n",
        "\n",
        "p = Pool(8)\n",
        "tokens = list(p.imap(cond_tokenize, combined_corpus))\n",
        "p.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wkE4KtB9OcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pure_tokens = [\" \".join(sent) for sent in tokens]\n",
        "print(tokens[0]) # this are the single lemmatized and stemmed tokens\n",
        "print(pure_tokens[0]) # these are the tokens combined in original form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43YC6wNF9052",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(min_df=1, max_features=2000, stop_words='english', ngram_range=[1, 1], sublinear_tf=True)\n",
        "tfidf = vectorizer.fit_transform(pure_tokens) # this is the vector matrix of the tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GxBZ2-6i97ak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idfs = pd.DataFrame([[v, k] for k, v in vectorizer.vocabulary_.items()], columns=['id', 'word']).sort_values('id')\n",
        "idfs['idf'] = vectorizer.idf_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWoiLhDp98sx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(idfs.sort_values('idf').head(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbOKcI09C6ri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tsvd = TruncatedSVD(n_components=500) # TODO this n_components=500 is a hyperparameter, look into it\n",
        "transformed = tsvd.fit_transform(tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B-I4R91ODBXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.sum(tsvd.explained_variance_ratio_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMWx2h1PDHg-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transformed.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qnlD02M-DgVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D = distance.pdist(transformed, 'cosine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WagL1YzhD8SH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "L = hierarchy.linkage(D)\n",
        "np.mean(D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GNfCjLsECpN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cls = hierarchy.fcluster(L, 0.71, criterion='inconsistent')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Fme4usWEU8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_cls = pd.DataFrame({'Pos': selected_ids, 'Cluster': cls})\n",
        "cnts = df_cls.groupby('Cluster').size().sort_values(ascending=False)\n",
        "cnts.sort_values(ascending=False).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ImQdm5aYEif0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc = pd.concat([sample, df_cls.set_index('Pos')], axis=1)\n",
        "bc.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-sKUng8E6gG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stats = bc.groupby('Cluster')['Score'].describe().unstack()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpNL9wdaFFHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stats.sort_values(ascending=False).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9erJFmPUFnLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.hlines([0], xmin=0, xmax=np.max(stats['count']) + 5, alpha=0.5)\n",
        "plt.vlines([1], ymin=0, ymax=np.max(stats['mean']) + 50, alpha=0.5)\n",
        "plt.scatter(stats['count'], stats['mean'], alpha=0.3)\n",
        "plt.title(\"cluster mean score vs cluster size\")\n",
        "plt.xlabel(\"cluster size\")\n",
        "plt.ylabel(\"mean score\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRNWyWL1HZ4I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[0]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJsIInooHgAj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[1]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJ7_d9jBHwLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bc.loc[bc['Cluster'] == cnts.index[2]][['Score', 'Title', 'Body']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DmuUVoP2GM5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kG7flNqaGdT7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions_df = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "tags_df = pd.read_csv(\"Tags.csv\", encoding='latin1')\n",
        "questions_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbGGD37kHHFr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob as tb\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "from html.parser import HTMLParser\n",
        "import collections\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "dataset_dir = \"../content/\"\n",
        "dataset_dir_questions = \"Questions.csv\"\n",
        "dataset_dir_answers = \"Answers.csv\"\n",
        "dataset_dir_tags = \"Tags.csv\"\n",
        "# for offline run\n",
        "# dataset_dir = \"/home/su/Downloads/stacksample\"\n",
        "# list the files in the dataset directory\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", dataset_dir]).decode(\"utf8\"))\n",
        "cachedStopWords = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuoDUdCgKtnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLStripper(HTMLParser):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        self.strict = False\n",
        "        self.convert_charrefs= True\n",
        "        self.fed = []\n",
        "    def handle_data(self, d):\n",
        "        self.fed.append(d)\n",
        "    def get_data(self):\n",
        "        return ''.join(self.fed)\n",
        "\n",
        "def strip_tags(html):\n",
        "    s = MLStripper()\n",
        "    s.feed(html)\n",
        "    return s.get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_TvkjYsK3sl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tf(word, blob):\n",
        "    return blob.words.count(word) / len(blob.words)\n",
        "def n_containing(word, bloblist):\n",
        "    return sum(1 for blob in bloblist if word in blob.words)\n",
        "def idf(word, bloblist):\n",
        "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
        "def tfidf(word, blob, bloblist):\n",
        "    return tf(word, blob) * idf(word, bloblist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aKnGap2MK3xa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(str):\n",
        "    return ' '.join([word for word in re.sub(r'[^\\w]', ' ', strip_tags(str)).lower().split() if word not in cachedStopWords and len(word) > 1 and not word.isdigit()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mv9HTSTLvO2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions_df = pd.read_csv(dataset_dir+dataset_dir_questions, encoding='latin1').iloc[::10000, :]\n",
        "answers_df = pd.read_csv(dataset_dir+dataset_dir_answers, encoding='latin1').iloc[::1000, :]\n",
        "tags_df = pd.read_csv(dataset_dir+dataset_dir_tags, encoding='latin1').iloc[::1000, :]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17BItKRoMfX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "questions_df.shape \n",
        "answers_df.shape \n",
        "tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10)\n",
        "answers_df.head(10)\n",
        "tags_df.head(10) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UORXP9K3NUNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "questions_df.shape \n",
        "answers_df.shape \n",
        "tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10).loc[:, 'Title':'Body']\n",
        "answers_df.head(10).loc[:, 'Body':]\n",
        "tags_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TF7V2nNqN0WV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for index, row in questions_df.iterrows():\n",
        "    questions_df.at[index, 'Body']= normalize(row[6])\n",
        "    questions_df.at[index, 'Title']= normalize(row[5])\n",
        "for index, row in answers_df.iterrows():\n",
        "    answers_df.at[index, 'Body']= normalize(row[5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k57NXs6YN9RY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Calculate dimensionality\n",
        "# questions_df.shape \n",
        "# answers_df.shape \n",
        "# tags_df.shape \n",
        "\n",
        "# Sample dataframe - uncomment to view\n",
        "questions_df.head(10).loc[:, 'Title':'Body']\n",
        "# answers_df.head(10).loc[:, 'Body':]\n",
        "# tags_df.head(10) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqlmLsfsODac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf_dict={}\n",
        "qID_dict={}\n",
        "bloblist=[]\n",
        "idlist=[]\n",
        "\n",
        "for index, row in questions_df.iterrows():\n",
        "    # also append title to text body\n",
        "    bloblist.append(tb(row[6]+\" \"+row[5]))\n",
        "    idlist.append(row[0])\n",
        "\n",
        "for i, blob in enumerate(bloblist):\n",
        "    if i < 5:\n",
        "        print(\"Top words in question ID {}\".format(idlist[i]))\n",
        "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
        "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, score in sorted_words[:5]:\n",
        "        if i < 5:\n",
        "            print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
        "            \n",
        "        # word dict    \n",
        "        if word in tfidf_dict:\n",
        "            tfidf_dict[word].append([idlist[i],round(score, 5)])\n",
        "        else:\n",
        "            tfidf_dict[word] = [[idlist[i],round(score, 5)]]\n",
        "        \n",
        "        # qID dict\n",
        "        if idlist[i] in qID_dict:\n",
        "            qID_dict[idlist[i]].append(word)\n",
        "        else:\n",
        "            lst=[]\n",
        "            lst.append(word)\n",
        "            qID_dict[idlist[i]]=lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2eMGSEdOesL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "for k, v in tfidf_dict.items():\n",
        "    print(k, v)\n",
        "    if i == 10:\n",
        "        break\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0IHwJtrOqke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_duplicate(query):\n",
        "    \n",
        "    def top_words(text):\n",
        "        counts = collections.Counter(text.split())\n",
        "        return [elem for elem, _ in counts.most_common(5)]\n",
        "\n",
        "    termList=top_words(query)\n",
        "    \n",
        "    for k, v in qID_dict.items():\n",
        "        if jaccard_similarity_score(termList, qID_dict[k]) >= 0.75:\n",
        "            print(\"Duplicate Question. Question exists with qID: \" + k)\n",
        "            return\n",
        "    \n",
        "print(\"Not a Duplicate Question.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFM_Hvo6PP1Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inputQ_title=\"What is the most efficient way to deep clone an object in JavaScript?\"\n",
        "inputQ_body=\"\"\"What is the most efficient way to clone a JavaScript object? \n",
        "I've seen obj = eval(uneval(o)); being used, \n",
        "but that's non-standard and only supported by Firefox.\n",
        "I've done things like obj = JSON.parse(JSON.stringify(o)); but question the efficiency. \n",
        "I've also seen recursive copying functions with various flaws. \n",
        "I'm surprised no canonical solution exists.\"\"\"\n",
        "inputQ_tags=\"javascript, json, object\"\n",
        "\n",
        "# normalize\n",
        "normalized_query=normalize(inputQ_title + \" \" + inputQ_body+ \" \" + inputQ_tags)\n",
        "\n",
        "# predict whether duplicate question\n",
        "predict_duplicate(normalized_query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oND2QVhpPkQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags_tally = collections.Counter(tags_df['Tag'])\n",
        "\n",
        "# x = tag name, y = tag frequency\n",
        "x, y = zip(*tags_tally.most_common(10))\n",
        "\n",
        "colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "area = [i/3 for i in list(y)]   # 0 to 15 point radiuses\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Top 10 most common tags\")\n",
        "for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='v', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "plt.legend(numpoints=1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgvAex2OQAT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ans_per_question = collections.Counter(answers_df['ParentId'])\n",
        "answerid,noAnswers= zip(*ans_per_question.most_common())\n",
        "\n",
        "N=50\n",
        "plt.bar(range(N), noAnswers[:N], align='center', alpha=0.7)\n",
        "#plt.xticks(y_pos, objects)\n",
        "\n",
        "plt.ylabel('Number of Answers per Question')\n",
        "plt.xlabel('Question Id')\n",
        "plt.title('Distribution of Answers per question ')\n",
        "plt.text(10,1.5,\"Average answers per question: \"+str(math.floor((np.mean(noAnswers)))))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2d9WJ8hjXO_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#word2vec embedding\n",
        "import csv\n",
        "outputDatafile = open('Tags.csv', 'r')\n",
        "outputReader = csv.reader(outputDatafile, delimiter=',', quoting=csv.QUOTE_NONE)\n",
        "\n",
        "print (\"OutputReader datatype :\",type(outputReader))\n",
        "inputData= []\n",
        "\n",
        "for row in outputReader:\n",
        "    inputData.append(row)\n",
        "    \n",
        "\n",
        "    print(inputData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4zkiFVifjqrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!NotebookApp.iopub_data_rate_limit=1000000000000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7xi1uZ-Bc_we",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "data,count, dictinory, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
        "\n",
        "window_size = 3\n",
        "vector_dim = 300\n",
        "epochs = 25\n",
        "\n",
        "valid_size = 16\n",
        "valid_window = 100\n",
        "valid_examples = np.random.choice(valid_window,valid_size,replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q2Bq0whHfmbc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sampling_table = sequence.make_sampling_table(vocab_size)\n",
        "couples, labels = skipgrams(data, vocab_size, window_size, sampling_table=sampling_table)\n",
        "word_target, word_context = zip(*couples)\n",
        "word_target = np.array(word_target,dtype=\"int32\")\n",
        "word_context = np.array(word_context,dtype=\"int32\")\n",
        "\n",
        "print(couples[:10], labels[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ny-MPhz7Ytck",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyAyxnRqYtEn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuEt9wkoYyc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"Tags.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcHaNP6iY67m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2YiClONZIkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"tags Characters: \", n_chars)\n",
        "print (\"tags Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9OLYIcasZcqG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"tags Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sufP0GSnrIgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "questions = pd.read_csv(\"Questions.csv\", encoding='latin1')\n",
        "answers = pd.read_csv(\"Answers.csv\", encoding='latin1')\n",
        "tags = pd.read_csv(\"Tags.csv\", encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYkuv7MGs_t3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions.shape\n",
        "answers.shape\n",
        "tags.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r9JvQhdPwBkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np \n",
        "import string\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1b1uilv-wC9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    global EMPTY\n",
        "    EMPTY = ''\n",
        "    \n",
        "    if not isinstance(text, str): \n",
        "        return text\n",
        "    text = re.sub('<pre><code>.*?</code></pre>', EMPTY, text)\n",
        "\n",
        "    def replace_link(match):\n",
        "        return EMPTY if re.match('[a-z]+://', match.group(1)) else match.group(1)\n",
        "    \n",
        "    text = re.sub('<a[^>]+>(.*)</a>', replace_link, text)\n",
        "    return re.sub('<[^>]+>', EMPTY, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9S44qLU3wGJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "questions['Text'] = questions['Body'].apply(clean_text).str.lower()\n",
        "questions.Text = questions.Text.apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KEpMofS_wWEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "answers['Text'] = answers['Body'].apply(clean_text).str.lower()\n",
        "answers.Text = answers.Text.apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZzV5TEiwiWr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'].nunique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7udxsPPEwuE7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_tags(tagCount):\n",
        "    \n",
        "    x,y = zip(*tagCount)\n",
        "\n",
        "    colormap = plt.cm.gist_ncar #nipy_spectral, Set1,Paired  \n",
        "    colors = [colormap(i) for i in np.linspace(0, 0.8,50)]   \n",
        "\n",
        "    area = [i/4000 for i in list(y)]   # 0 to 15 point radiuses\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.ylabel(\"Number of question associations\")\n",
        "    for i in range(len(y)):\n",
        "        plt.plot(i,y[i], marker='o', linestyle='',ms=area[i],label=x[i])\n",
        "\n",
        "    plt.legend(numpoints=1)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kaCNm3Yjwvve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "tagCount =  collections.Counter(list(tags['Tag'])).most_common(10)\n",
        "print(tagCount)\n",
        "plot_tags(tagCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ILSj9sw2w5or",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags = tags[(tags.Tag == 'c#') | (tags.Tag == 'java') | (tags.Tag == 'php') | (tags.Tag =='javascript') | (tags.Tag =='jquery') | (tags.Tag == 'android') | (tags.Tag == 'c++') | (tags.Tag == 'iphone') | (tags.Tag == 'python') | (tags.Tag == 'asp.net')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7z3_MmMKxJvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_qo0y6Hx1TD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/stack_over_flow_auto_tagging.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "HcmGBy8Mx3I2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"../content/Questions.csv\", encoding=\"ISO-8859-1\")\n",
        "tags = pd.read_csv(\"../content/Tags.csv\", encoding=\"ISO-8859-1\", dtype={'Tag': str})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmrlsFgIvtQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags['Tag'] = tags['Tag'].astype(str)\n",
        "grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n",
        "grouped_tags.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "emF6pIquv6Dm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped_tags.reset_index()\n",
        "grouped_tags_final = pd.DataFrame({'Id':grouped_tags.index, 'Tags':grouped_tags.values})\n",
        "grouped_tags_final.head(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4MW7KCOwCRK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.drop(columns=['OwnerUserId', 'CreationDate', 'ClosedDate'], inplace=True)\n",
        "df = df.merge(grouped_tags_final, on='Id')\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2tuK_1yowLIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df = df[df['Score']>3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HTQS04K5wRZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "new_df.isnull().mean(axis=0).plot.barh()\n",
        "plt.title(\"Ratio of missing values per columns\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Db_3E-xfwe76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Dupplicate entries: {}'.format(new_df.duplicated().sum()))\n",
        "new_df.drop_duplicates(inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8YKRmzK0wjPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmuNDB6Iw4_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: x.split())\n",
        "all_tags = [item for sublist in new_df['Tags'].values for item in sublist]\n",
        "len(all_tags)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pW0Y_YsaxiVo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "my_set = set(all_tags)\n",
        "unique_tags = list(my_set)\n",
        "len(unique_tags)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUCfgJWOxpAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "flat_list = [item for sublist in new_df['Tags'].values for item in sublist]\n",
        "\n",
        "keywords = nltk.FreqDist(flat_list)\n",
        "\n",
        "keywords = nltk.FreqDist(keywords)\n",
        "\n",
        "frequencies_words = keywords.most_common(100)\n",
        "tags_features = [word[0] for word in frequencies_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lH2YrZfsxyL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_CjNCyPYx4ci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "keywords.plot(100, cumulative=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ECmU4-hx6w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def most_common(tags):\n",
        "    tags_filtered = []\n",
        "    for i in range(0, len(tags)):\n",
        "        if tags[i] in tags_features:\n",
        "            tags_filtered.append(tags[i])\n",
        "    return tags_filtered"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NYI-RMEyDYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: most_common(x))\n",
        "new_df['Tags'] = new_df['Tags'].apply(lambda x: x if len(x)>0 else None)\n",
        "new_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gSZ4inXyJOq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df.dropna(subset=['Tags'], inplace=True)\n",
        "new_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PrRRSkPAyNr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: BeautifulSoup(x).get_text()) \n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
        "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip(' ')\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJxdNPQ4zDxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: clean_text(x)) \n",
        "token=ToktokTokenizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "APUrD61Qzb1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "punct = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WA1lxllCze2o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def strip_list_noempty(mylist):\n",
        "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
        "    return [item for item in newlist if item != '']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDVr65jOzgCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_punct(text): \n",
        "    words=token.tokenize(text)\n",
        "    punctuation_filtered = []\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
        "    for w in words:\n",
        "        if w in tags_features:\n",
        "            punctuation_filtered.append(w)\n",
        "        else:\n",
        "            punctuation_filtered.append(regex.sub('', w))\n",
        "  \n",
        "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
        "        \n",
        "    return ' '.join(map(str, filtered_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLpyBb_uzk2A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: clean_punct(x)) \n",
        "new_df['Body'][2]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7maDEj-y0lt3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "lemma=WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yPXVSZl1_jA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemitizeWords(text):\n",
        "    words=token.tokenize(text)\n",
        "    listLemma=[]\n",
        "    for w in words:\n",
        "        x=lemma.lemmatize(w, pos=\"v\")\n",
        "        listLemma.append(x)\n",
        "    return ' '.join(map(str, listLemma))\n",
        "\n",
        "def stopWordsRemove(text):\n",
        "    \n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    words=token.tokenize(text)\n",
        "    \n",
        "    filtered = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    return ' '.join(map(str, filtered))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BP6SdPSb2DaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: lemitizeWords(x)) \n",
        "new_df['Body'] = new_df['Body'].apply(lambda x: stopWordsRemove(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7R3D7gr221O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df['Title'] = new_df['Title'].apply(lambda x: str(x))\n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: clean_text(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: clean_punct(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: lemitizeWords(x)) \n",
        "new_df['Title'] = new_df['Title'].apply(lambda x: stopWordsRemove(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMbKb21n3ItH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "no_topics = 40\n",
        "text = new_df['Body']\n",
        "vectorizer_train = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n",
        "                                       max_features=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqrAqSfX3Vfl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "TF_IDF_matrix = vectorizer_train.fit_transform(text)\n",
        "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50,random_state=11).fit(TF_IDF_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CeHZmmtO44v6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OjkcgC3249Wm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "no_top_words = 20\n",
        "display_topics(lda, vectorizer_train.get_feature_names(), no_top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mepWdzGl5HW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "X1 = new_df['Body']\n",
        "X2 = new_df['Title']\n",
        "y = new_df['Tags']\n",
        "\n",
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "y_bin = multilabel_binarizer.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "joMDLwNL5PGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
        "                                       max_features=1000)\n",
        "\n",
        "vectorizer_X2 = TfidfVectorizer(analyzer = 'word',\n",
        "                                       min_df=0.0,\n",
        "                                       max_df = 1.0,\n",
        "                                       strip_accents = None,\n",
        "                                       encoding = 'utf-8', \n",
        "                                       preprocessor=None,\n",
        "                                       token_pattern=r\"(?u)\\S\\S+\",\n",
        "                                       max_features=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TjIz9QT1_dUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****Model training and testing****"
      ]
    },
    {
      "metadata": {
        "id": "sIwXBtoq5SyR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X1_tfidf = vectorizer_X1.fit_transform(X1)\n",
        "X2_tfidf = vectorizer_X2.fit_transform(X2)\n",
        "\n",
        "#X_tfidf.fit(np.transpose(np.matrix(X1_tfidf)), np.transpose(np.matrix(X2_tfidf)))\n",
        "X_tfidf = np.stack((np.matrix(X1_tfidf),np.matrix(X2_tfidf)), axis=0)\n",
        "#X_tfidf.shape\n",
        "#y_bin.shape\n",
        "print(X_tfidf.shape,y_bin.shape)\n",
        "print(X1_tfidf.shape,X2_tfidf.shape)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_bin, test_size = 0.2, random_state = 0) # Do 80/20 split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IAGrX3QE7cS5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def avg_jacard(y_true,y_pred):\n",
        "    \n",
        "    jacard = np.minimum(y_true,y_pred).sum(axis=1) / np.maximum(y_true,y_pred).sum(axis=1)\n",
        "    \n",
        "    return jacard.mean()*100\n",
        "\n",
        "def print_score(y_pred, clf):\n",
        "    print(\"Clf: \", clf.__class__.__name__)\n",
        "    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n",
        "    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n",
        "    print(\"---\")    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_9jaI7y7jE4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "\n",
        "dummy = DummyClassifier()\n",
        "sgd = SGDClassifier()\n",
        "lr = LogisticRegression()\n",
        "mn = MultinomialNB()\n",
        "svc = LinearSVC()\n",
        "perceptron = Perceptron()\n",
        "pac = PassiveAggressiveClassifier()\n",
        "\n",
        "for classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n",
        "    clf = OneVsRestClassifier(classifier)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print_score(y_pred, classifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1V9b3BXV-N_l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlpc = MLPClassifier()\n",
        "mlpc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlpc.predict(X_test)\n",
        "\n",
        "print_score(y_pred, mlpc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eGvNd_Wz-TEA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mlpc = MLPClassifier()\n",
        "mlpc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlpc.predict(X_test)\n",
        "\n",
        "print_score(y_pred, mlpc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VX9OhMdq-aHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "param_grid = {'estimator__C':[1,10,100,1000]\n",
        "              }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14bm064P-mXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PAt5QXp--bkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "svc = OneVsRestClassifier(LinearSVC())\n",
        "CV_svc = model_selection.GridSearchCV(estimator=svc, param_grid=param_grid, cv= 5, verbose=10, scoring=make_scorer(avg_jacard,greater_is_better=True))\n",
        "CV_svc.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IMtUYRGx_EDa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "CV_svc.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jghx-9vU_MoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_model = CV_svc.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print_score(y_pred, best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o5UB65zN_lyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****Model end****"
      ]
    },
    {
      "metadata": {
        "id": "Nc6DuROD_pI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(y_train.shape[1]):\n",
        "    print(multilabel_binarizer.classes_[i])\n",
        "    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n",
        "    print(\"\")def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"%s: %s\" % (class_label,\n",
        "              \" \".join(feature_names[j] for j in top10)))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n4vlo5xp_urW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_top10(feature_names, clf, class_labels):\n",
        "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
        "    for i, class_label in enumerate(class_labels):\n",
        "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
        "        print(\"--------------------------------------------\")\n",
        "        print(\"%s: %s\" % (class_label,\n",
        "              \" \".join(feature_names[j] for j in top10)))\n",
        "        print(\"--------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wm8pgZ0m_zTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras, hyperopt, tensorflow\n",
        "\n",
        "\n",
        "feature_names = vectorizer_X1.get_feature_names() + vectorizer_X2.get_feature_names()\n",
        "print_top10(feature_names, best_model, multilabel_binarizer.classes_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F4-8L8DABxQL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!NEW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
      ]
    },
    {
      "metadata": {
        "id": "T3ChjcHKBwWj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import os\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uwn_Okn8HZVi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "88797a87-c7a2-4762-f2f2-69175e644ddb"
      },
      "cell_type": "code",
      "source": [
        "  tag_dtm = vectorizer.fit_transform(tags['tag'])\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-59aeb583bad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtag_dtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jd-23CkyI3E6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}